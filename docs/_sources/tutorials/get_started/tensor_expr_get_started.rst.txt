
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/get_started/tensor_expr_get_started.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorials_get_started_tensor_expr_get_started.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_get_started_tensor_expr_get_started.py:


.. _tutorial-tensor-expr-get-started:

Get Started with Tensor Expression
==================================
**Author**: `Tianqi Chen <https://tqchen.github.io>`_

This is an introductory tutorial to the Tensor expression language in TVM.
TVM uses a domain specific tensor expression for efficient kernel construction.

In this tutorial, we will demonstrate the basic workflow to use
the tensor expression language.

.. GENERATED FROM PYTHON SOURCE LINES 30-43

.. code-block:: default

    from __future__ import absolute_import, print_function

    import tvm
    import tvm.testing
    from tvm import te
    import numpy as np

    # Global declarations of environment.

    tgt_host = "llvm"
    # Change it to respective GPU if gpu is enabled Ex: cuda, opencl, rocm
    tgt = "cuda"








.. GENERATED FROM PYTHON SOURCE LINES 44-49

Vector Add Example
------------------
In this tutorial, we will use a vector addition example to demonstrate
the workflow.


.. GENERATED FROM PYTHON SOURCE LINES 51-69

Describe the Computation
------------------------
As a first step, we need to describe our computation.
TVM adopts tensor semantics, with each intermediate result
represented as a multi-dimensional array. The user needs to describe
the computation rule that generates the tensors.

We first define a symbolic variable n to represent the shape.
We then define two placeholder Tensors, A and B, with given shape (n,)

We then describe the result tensor C, with a compute operation.  The
compute function takes the shape of the tensor, as well as a lambda
function that describes the computation rule for each position of
the tensor.

No computation happens during this phase, as we are only declaring how
the computation should be done.


.. GENERATED FROM PYTHON SOURCE LINES 69-75

.. code-block:: default

    n = te.var("n")
    A = te.placeholder((n,), name="A")
    B = te.placeholder((n,), name="B")
    C = te.compute(A.shape, lambda i: A[i] + B[i], name="C")
    print(type(C))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    <class 'tvm.te.tensor.Tensor'>




.. GENERATED FROM PYTHON SOURCE LINES 76-95

Schedule the Computation
------------------------
While the above lines describe the computation rule, we can compute
C in many ways since the axis of C can be computed in a data
parallel manner.  TVM asks the user to provide a description of the
computation called a schedule.

A schedule is a set of transformation of computation that transforms
the loop of computations in the program.

After we construct the schedule, by default the schedule computes
C in a serial manner in a row-major order.

.. code-block:: c

  for (int i = 0; i < n; ++i) {
    C[i] = A[i] + B[i];
  }


.. GENERATED FROM PYTHON SOURCE LINES 95-97

.. code-block:: default

    s = te.create_schedule(C.op)








.. GENERATED FROM PYTHON SOURCE LINES 98-113

We used the split construct to split the first axis of C,
this will split the original iteration axis into product of
two iterations. This is equivalent to the following code.

.. code-block:: c

  for (int bx = 0; bx < ceil(n / 64); ++bx) {
    for (int tx = 0; tx < 64; ++tx) {
      int i = bx * 64 + tx;
      if (i < n) {
        C[i] = A[i] + B[i];
      }
    }
  }


.. GENERATED FROM PYTHON SOURCE LINES 113-115

.. code-block:: default

    bx, tx = s[C].split(C.op.axis[0], factor=64)








.. GENERATED FROM PYTHON SOURCE LINES 116-120

Finally we bind the iteration axis bx and tx to threads in the GPU
compute grid. These are GPU specific constructs that allow us
to generate code that runs on GPU.


.. GENERATED FROM PYTHON SOURCE LINES 120-124

.. code-block:: default

    if tgt == "cuda" or tgt == "rocm" or tgt.startswith("opencl"):
        s[C].bind(bx, te.thread_axis("blockIdx.x"))
        s[C].bind(tx, te.thread_axis("threadIdx.x"))








.. GENERATED FROM PYTHON SOURCE LINES 125-141

Compilation
-----------
After we have finished specifying the schedule, we can compile it
into a TVM function. By default TVM compiles into a type-erased
function that can be directly called from the python side.

In the following line, we use tvm.build to create a function.
The build function takes the schedule, the desired signature of the
function (including the inputs and outputs) as well as target language
we want to compile to.

The result of compilation fadd is a GPU device function (if GPU is
involved) as well as a host wrapper that calls into the GPU
function.  fadd is the generated host wrapper function, it contains
a reference to the generated device function internally.


.. GENERATED FROM PYTHON SOURCE LINES 141-143

.. code-block:: default

    fadd = tvm.build(s, [A, B, C], tgt, target_host=tgt_host, name="myadd")



.. rst-class:: sphx-glr-script-out

.. code-block:: pytb

    Traceback (most recent call last):
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/tutorials/get_started/tensor_expr_get_started.py", line 141, in <module>
        fadd = tvm.build(s, [A, B, C], tgt, target_host=tgt_host, name="myadd")
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/driver/build_module.py", line 416, in build
        mod_host, mdev = _build_for_device(input_mod, tar, target_host)
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/driver/build_module.py", line 297, in _build_for_device
        rt_mod_dev = codegen.build_module(mod_dev, target) if len(mod_dev.functions) != 0 else None
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/target/codegen.py", line 39, in build_module
        return _ffi_api.Build(mod, target)
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
        raise get_last_ffi_error()
    tvm._ffi.base.TVMError: Traceback (most recent call last):
      [bt] (3) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(TVMFuncCall+0x65) [0x7f14f98ff845]
      [bt] (2) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target))::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0x677) [0x7f14f9299617]
      [bt] (1) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(tvm::codegen::Build(tvm::IRModule, tvm::Target)+0xe62) [0x7f14f9292f92]
      [bt] (0) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(+0xba1e42) [0x7f14f9291e42]
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/src/target/codegen.cc", line 58
    TVMError: 
    ---------------------------------------------------------------
    An internal invariant was violated during the execution of TVM.
    Please read TVM's error reporting guidelines.
    More details can be found here: https://discuss.tvm.ai/t/error-reporting/7793.
    ---------------------------------------------------------------
      Check failed: bf != nullptr == false: target.build.cuda is not enabled




.. GENERATED FROM PYTHON SOURCE LINES 144-157

Run the Function
----------------
The compiled TVM function is exposes a concise C API
that can be invoked from any language.

We provide a minimal array API in python to aid quick testing and prototyping.
The array API is based on the `DLPack <https://github.com/dmlc/dlpack>`_ standard.

- We first create a GPU context.
- Then tvm.nd.array copies the data to the GPU.
- fadd runs the actual computation.
- asnumpy() copies the GPU array back to the CPU and we can use this to verify correctness


.. GENERATED FROM PYTHON SOURCE LINES 157-166

.. code-block:: default

    ctx = tvm.context(tgt, 0)

    n = 1024
    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), ctx)
    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), ctx)
    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), ctx)
    fadd(a, b, c)
    tvm.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())


.. GENERATED FROM PYTHON SOURCE LINES 167-175

Inspect the Generated Code
--------------------------
You can inspect the generated code in TVM. The result of tvm.build
is a TVM Module. fadd is the host module that contains the host wrapper,
it also contains a device module for the CUDA (GPU) function.

The following code fetches the device module and prints the content code.


.. GENERATED FROM PYTHON SOURCE LINES 175-182

.. code-block:: default

    if tgt == "cuda" or tgt == "rocm" or tgt.startswith("opencl"):
        dev_module = fadd.imported_modules[0]
        print("-----GPU code-----")
        print(dev_module.get_source())
    else:
        print(fadd.get_source())


.. GENERATED FROM PYTHON SOURCE LINES 183-199

.. note:: Code Specialization

  As you may have noticed, the declarations of A, B and C all
  take the same shape argument, n. TVM will take advantage of this
  to pass only a single shape argument to the kernel, as you will find in
  the printed device code. This is one form of specialization.

  On the host side, TVM will automatically generate check code
  that checks the constraints in the parameters. So if you pass
  arrays with different shapes into fadd, an error will be raised.

  We can do more specializations. For example, we can write
  :code:`n = tvm.runtime.convert(1024)` instead of :code:`n = te.var("n")`,
  in the computation declaration. The generated function will
  only take vectors with length 1024.


.. GENERATED FROM PYTHON SOURCE LINES 201-212

Save Compiled Module
--------------------
Besides runtime compilation, we can save the compiled modules into
a file and load them back later. This is called ahead of time compilation.

The following code first performs the following steps:

- It saves the compiled host module into an object file.
- Then it saves the device module into a ptx file.
- cc.create_shared calls a compiler (gcc) to create a shared library


.. GENERATED FROM PYTHON SOURCE LINES 212-226

.. code-block:: default

    from tvm.contrib import cc
    from tvm.contrib import utils

    temp = utils.tempdir()
    fadd.save(temp.relpath("myadd.o"))
    if tgt == "cuda":
        fadd.imported_modules[0].save(temp.relpath("myadd.ptx"))
    if tgt == "rocm":
        fadd.imported_modules[0].save(temp.relpath("myadd.hsaco"))
    if tgt.startswith("opencl"):
        fadd.imported_modules[0].save(temp.relpath("myadd.cl"))
    cc.create_shared(temp.relpath("myadd.so"), [temp.relpath("myadd.o")])
    print(temp.listdir())


.. GENERATED FROM PYTHON SOURCE LINES 227-234

.. note:: Module Storage Format

  The CPU (host) module is directly saved as a shared library (.so).
  There can be multiple customized formats of the device code.
  In our example, the device code is stored in ptx, as well as a meta
  data json file. They can be loaded and linked separately via import.


.. GENERATED FROM PYTHON SOURCE LINES 236-242

Load Compiled Module
--------------------
We can load the compiled module from the file system and run the code.
The following code loads the host and device module separately and
re-links them together. We can verify that the newly loaded function works.


.. GENERATED FROM PYTHON SOURCE LINES 242-258

.. code-block:: default

    fadd1 = tvm.runtime.load_module(temp.relpath("myadd.so"))
    if tgt == "cuda":
        fadd1_dev = tvm.runtime.load_module(temp.relpath("myadd.ptx"))
        fadd1.import_module(fadd1_dev)

    if tgt == "rocm":
        fadd1_dev = tvm.runtime.load_module(temp.relpath("myadd.hsaco"))
        fadd1.import_module(fadd1_dev)

    if tgt.startswith("opencl"):
        fadd1_dev = tvm.runtime.load_module(temp.relpath("myadd.cl"))
        fadd1.import_module(fadd1_dev)

    fadd1(a, b, c)
    tvm.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())


.. GENERATED FROM PYTHON SOURCE LINES 259-267

Pack Everything into One Library
--------------------------------
In the above example, we store the device and host code separately.
TVM also supports export everything as one shared library.
Under the hood, we pack the device modules into binary blobs and link
them together with the host code.
Currently we support packing of Metal, OpenCL and CUDA modules.


.. GENERATED FROM PYTHON SOURCE LINES 267-272

.. code-block:: default

    fadd.export_library(temp.relpath("myadd_pack.so"))
    fadd2 = tvm.runtime.load_module(temp.relpath("myadd_pack.so"))
    fadd2(a, b, c)
    tvm.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())


.. GENERATED FROM PYTHON SOURCE LINES 273-283

.. note:: Runtime API and Thread-Safety

  The compiled modules of TVM do not depend on the TVM compiler.
  Instead, they only depend on a minimum runtime library.
  The TVM runtime library wraps the device drivers and provides
  thread-safe and device agnostic calls into the compiled functions.

  This means that you can call the compiled TVM functions from any thread,
  on any GPUs.


.. GENERATED FROM PYTHON SOURCE LINES 285-293

Generate OpenCL Code
--------------------
TVM provides code generation features into multiple backends,
we can also generate OpenCL code or LLVM code that runs on CPU backends.

The following code blocks generate OpenCL code, creates array on an OpenCL
device, and verifies the correctness of the code.


.. GENERATED FROM PYTHON SOURCE LINES 293-305

.. code-block:: default

    if tgt.startswith("opencl"):
        fadd_cl = tvm.build(s, [A, B, C], tgt, name="myadd")
        print("------opencl code------")
        print(fadd_cl.imported_modules[0].get_source())
        ctx = tvm.cl(0)
        n = 1024
        a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), ctx)
        b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), ctx)
        c = tvm.nd.array(np.zeros(n, dtype=C.dtype), ctx)
        fadd_cl(a, b, c)
        tvm.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())


.. GENERATED FROM PYTHON SOURCE LINES 306-320

Summary
-------
This tutorial provides a walk through of TVM workflow using
a vector add example. The general workflow is

- Describe your computation via a series of operations.
- Describe how we want to compute use schedule primitives.
- Compile to the target function we want.
- Optionally, save the function to be loaded later.

You are more than welcome to checkout other examples and
tutorials to learn more about the supported operations, scheduling primitives
and other features in TVM.



.. _sphx_glr_download_tutorials_get_started_tensor_expr_get_started.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: tensor_expr_get_started.py <tensor_expr_get_started.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: tensor_expr_get_started.ipynb <tensor_expr_get_started.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
