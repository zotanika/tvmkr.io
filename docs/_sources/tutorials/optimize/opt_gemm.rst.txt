
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/optimize/opt_gemm.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorials_optimize_opt_gemm.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_optimize_opt_gemm.py:


.. _opt-gemm:

How to optimize GEMM on CPU
===========================
**Author**: `Jian Weng <https://github.com/were>`_,             `Ruofei Yu <https://github.com/yuruofeifei>`_

(TL;DR) TVM provides abstract interfaces which allows users to depict an algorithm and the
algorithm's implementing organization (the so-called schedule) separately. Typically, writing
algorithm in high-performance schedule breaks the algorithm's readability and modularity. Also,
trying various seemingly promising schedules is time-consuming. With the help of TVM, we can
try these schedules efficiently to enhance the performance.

In this tutorial, we will demonstrate how to use TVM to optimize square matrix multiplication
and achieve 200 times faster than baseline by simply adding 18 extra lines of code.

There are two important optimizations on intense computation applications executed on CPU:
    1. Increase the cache hit rate of memory access. Both complex numerical computation and hot-spot
       memory access can be accelerated from high cache hit rate. This requires us to transform the
       origin memory access pattern to the pattern fits the cache policy.
    2. SIMD (Single instruction multi-data), or we call it vector processing unit. Every time, a
       small batch of data, rather than a single grid, will be processed. This requires us to
       transform the data access pattern in the loop body in uniform pattern so that the LLVM
       backend can lower it to SIMD.

Actually, all the methodologies used in this tutorial is a subset of tricks mentioned in this
`repo <https://github.com/flame/how-to-optimize-gemm>`_. Some of them have been applied by TVM
abstraction automatically, but some of them cannot be simply applied due to TVM constraints.

All the experiment results mentioned below, are executed on 2015's 15' MacBook equipped with
Intel i7-4770HQ CPU. The cache line size should be 64 bytes for all the x86 CPUs.

.. GENERATED FROM PYTHON SOURCE LINES 52-57

Preparation and Baseline
------------------------
In this tutorial, we will demo how to use TVM to optimize matrix multiplication.
Before actually demonstrating, we first define these variables.
Then we write a baseline implementation, the simplest way to write a matrix multiplication in TVM.

.. GENERATED FROM PYTHON SOURCE LINES 57-118

.. code-block:: default


    import tvm
    import tvm.testing
    from tvm import te
    import numpy
    import timeit

    # The size of the matrix
    # (M, K) x (K, N)
    # You are free to try out different shapes, sometimes TVM optimization outperforms numpy with MKL.
    M = 1024
    K = 1024
    N = 1024

    # The default tensor type in tvm
    dtype = "float32"

    # using Intel AVX2(Advanced Vector Extensions) ISA for SIMD
    # To get the best performance, please change the following line
    # to llvm -mcpu=core-avx2, or specific type of CPU you use
    target = "llvm"
    ctx = tvm.context(target, 0)

    # Random generated tensor for testing
    a = tvm.nd.array(numpy.random.rand(M, K).astype(dtype), ctx)
    b = tvm.nd.array(numpy.random.rand(K, N).astype(dtype), ctx)

    np_repeat = 100
    np_runing_time = timeit.timeit(
        setup="import numpy\n"
        "M = " + str(M) + "\n"
        "K = " + str(K) + "\n"
        "N = " + str(N) + "\n"
        'dtype = "float32"\n'
        "a = numpy.random.rand(M, K).astype(dtype)\n"
        "b = numpy.random.rand(K, N).astype(dtype)\n",
        stmt="answer = numpy.dot(a, b)",
        number=np_repeat,
    )
    print("Numpy running time: %f" % (np_runing_time / np_repeat))

    answer = numpy.dot(a.asnumpy(), b.asnumpy())

    # Algorithm
    k = te.reduce_axis((0, K), "k")
    A = te.placeholder((M, K), name="A")
    B = te.placeholder((K, N), name="B")
    C = te.compute((M, N), lambda x, y: te.sum(A[x, k] * B[k, y], axis=k), name="C")

    # Default schedule
    s = te.create_schedule(C.op)
    func = tvm.build(s, [A, B, C], target=target, name="mmult")
    assert func

    c = tvm.nd.array(numpy.zeros((M, N), dtype=dtype), ctx)
    func(a, b, c)
    tvm.testing.assert_allclose(c.asnumpy(), answer, rtol=1e-5)

    evaluator = func.time_evaluator(func.entry_name, ctx, number=1)
    print("Baseline: %f" % evaluator(a, b, c).mean)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Numpy running time: 0.008782
    Baseline: 1.475590




.. GENERATED FROM PYTHON SOURCE LINES 119-121

In TVM, we can always inspect lower level IR to debug or optimize our schedule.
Here is the generated IR using our baseline schedule.

.. GENERATED FROM PYTHON SOURCE LINES 121-124

.. code-block:: default


    print(tvm.lower(s, [A, B, C], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(A_1: handle, B_1: handle, C_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], []),
                 A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),
                 B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], [])}
      buffer_map = {A_1: A, B_1: B, C_1: C} {
      for (x: int32, 0, 1024) {
        for (y: int32, 0, 1024) {
          C_2[((x*1024) + y)] = 0f32
          for (k: int32, 0, 1024) {
            C_2[((x*1024) + y)] = ((float32*)C_2[((x*1024) + y)] + ((float32*)A_2[((x*1024) + k)]*(float32*)B_2[((k*1024) + y)]))
          }
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 125-131

Blocking
--------
A important trick to enhance the cache hit rate is blocking --- data chunk will be computed
block by block. The memory access inside the block is a small neighbourhood which is with high
memory locality. In this tutorial, I picked up 32 as the blocking factor. So the block will
fill 32 * 32 * sizeof(float) which is 4KB in the cache whose total size is 32KB (L1 data cache)

.. GENERATED FROM PYTHON SOURCE LINES 131-155

.. code-block:: default


    bn = 32
    s = te.create_schedule(C.op)

    # Blocking by loop tiling
    xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)
    (k,) = s[C].op.reduce_axis
    ko, ki = s[C].split(k, factor=4)

    # Hoist reduction domain outside the blocking loop
    s[C].reorder(xo, yo, ko, ki, xi, yi)

    func = tvm.build(s, [A, B, C], target=target, name="mmult")
    assert func

    c = tvm.nd.array(numpy.zeros((M, N), dtype=dtype), ctx)
    func(a, b, c)
    tvm.testing.assert_allclose(c.asnumpy(), answer, rtol=1e-5)

    # By simply tiling the loop 32x32, and hoisting ko, ki outside the blocking loops,
    # we can see big speedup compared with the baseline.
    evaluator = func.time_evaluator(func.entry_name, ctx, number=10)
    print("Opt1: %f" % evaluator(a, b, c).mean)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Opt1: 0.210014




.. GENERATED FROM PYTHON SOURCE LINES 156-157

Here is the generated IR after blocking.

.. GENERATED FROM PYTHON SOURCE LINES 157-160

.. code-block:: default


    print(tvm.lower(s, [A, B, C], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(A_1: handle, B_1: handle, C_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], []),
                 A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),
                 B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], [])}
      buffer_map = {A_1: A, B_1: B, C_1: C} {
      for (x.outer: int32, 0, 32) {
        for (y.outer: int32, 0, 32) {
          for (x.inner.init: int32, 0, 32) {
            for (y.inner.init: int32, 0, 32) {
              C_2[((((x.outer*32768) + (x.inner.init*1024)) + (y.outer*32)) + y.inner.init)] = 0f32
            }
          }
          for (k.outer: int32, 0, 256) {
            for (k.inner: int32, 0, 4) {
              for (x.inner: int32, 0, 32) {
                for (y.inner: int32, 0, 32) {
                  C_2[((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)) + y.inner)] = ((float32*)C_2[((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)) + y.inner)] + ((float32*)A_2[((((x.outer*32768) + (x.inner*1024)) + (k.outer*4)) + k.inner)]*(float32*)B_2[((((k.outer*4096) + (k.inner*1024)) + (y.outer*32)) + y.inner)]))
                }
              }
            }
          }
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 161-168

Vectorization
-------------
Another important trick is vectorization. When the memory access pattern is uniform,
the compiler can detect this pattern and pass the continuous memory to vector processor. In TVM,
we can use `vectorize` interface to hint the compiler this pattern, so that we can accelerate it vastly.

In this tutorial, we chose to vectorize the inner loop row data since it is cache friendly.

.. GENERATED FROM PYTHON SOURCE LINES 168-189

.. code-block:: default


    s = te.create_schedule(C.op)
    xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)
    (k,) = s[C].op.reduce_axis
    ko, ki = s[C].split(k, factor=4)

    s[C].reorder(xo, yo, ko, ki, xi, yi)

    # Vectorization
    s[C].vectorize(yi)

    func = tvm.build(s, [A, B, C], target=target, name="mmult")
    assert func

    c = tvm.nd.array(numpy.zeros((M, N), dtype=dtype), ctx)
    func(a, b, c)
    tvm.testing.assert_allclose(c.asnumpy(), answer, rtol=1e-5)

    evaluator = func.time_evaluator(func.entry_name, ctx, number=10)
    print("Opt2: %f" % evaluator(a, b, c).mean)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Opt2: 0.229554




.. GENERATED FROM PYTHON SOURCE LINES 190-191

Here is the generated IR after vectorization.

.. GENERATED FROM PYTHON SOURCE LINES 191-194

.. code-block:: default


    print(tvm.lower(s, [A, B, C], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(A_1: handle, B_1: handle, C_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], []),
                 A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),
                 B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], [])}
      buffer_map = {A_1: A, B_1: B, C_1: C} {
      for (x.outer: int32, 0, 32) {
        for (y.outer: int32, 0, 32) {
          for (x.inner.init: int32, 0, 32) {
            C_2[ramp((((x.outer*32768) + (x.inner.init*1024)) + (y.outer*32)), 1, 32)] = broadcast(0f32, 32)
          }
          for (k.outer: int32, 0, 256) {
            for (k.inner: int32, 0, 4) {
              for (x.inner: int32, 0, 32) {
                C_2[ramp((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)), 1, 32)] = ((float32x32*)C_2[ramp((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.inner*1024)) + (k.outer*4)) + k.inner)], 32)*(float32x32*)B_2[ramp((((k.outer*4096) + (k.inner*1024)) + (y.outer*32)), 1, 32)]))
              }
            }
          }
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 195-202

Loop Permutation
----------------
If we look at the above IR, we can see the inner loop row data is vectorized and
B is transformed into PackedB. The traversal of PackedB is sequential now.
So we will look at the access pattern of A. In current schedule, A is accessed column by column
which is not cache friendly. If we change the nested loop order of ki and inner axes xi,
the access pattern for A matrix is more cache friendly.

.. GENERATED FROM PYTHON SOURCE LINES 202-222

.. code-block:: default


    s = te.create_schedule(C.op)
    xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)
    (k,) = s[C].op.reduce_axis
    ko, ki = s[C].split(k, factor=4)

    # re-ordering
    s[C].reorder(xo, yo, ko, xi, ki, yi)
    s[C].vectorize(yi)

    func = tvm.build(s, [A, B, C], target=target, name="mmult")
    assert func

    c = tvm.nd.array(numpy.zeros((M, N), dtype=dtype), ctx)
    func(a, b, c)
    tvm.testing.assert_allclose(c.asnumpy(), answer, rtol=1e-5)

    evaluator = func.time_evaluator(func.entry_name, ctx, number=10)
    print("Opt3: %f" % evaluator(a, b, c).mean)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Opt3: 0.081923




.. GENERATED FROM PYTHON SOURCE LINES 223-224

Here is the generated IR after loop permutation.

.. GENERATED FROM PYTHON SOURCE LINES 224-227

.. code-block:: default


    print(tvm.lower(s, [A, B, C], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(A_1: handle, B_1: handle, C_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], []),
                 A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),
                 B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], [])}
      buffer_map = {A_1: A, B_1: B, C_1: C} {
      for (x.outer: int32, 0, 32) {
        for (y.outer: int32, 0, 32) {
          for (x.inner.init: int32, 0, 32) {
            C_2[ramp((((x.outer*32768) + (x.inner.init*1024)) + (y.outer*32)), 1, 32)] = broadcast(0f32, 32)
          }
          for (k.outer: int32, 0, 256) {
            for (x.inner: int32, 0, 32) {
              for (k.inner: int32, 0, 4) {
                C_2[ramp((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)), 1, 32)] = ((float32x32*)C_2[ramp((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.inner*1024)) + (k.outer*4)) + k.inner)], 32)*(float32x32*)B_2[ramp((((k.outer*4096) + (k.inner*1024)) + (y.outer*32)), 1, 32)]))
              }
            }
          }
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 228-237

Array Packing
-------------
Another important trick is array packing. This trick is to reorder the storage dimension of the
array to convert the continuous access pattern on certain dimension to a sequential pattern after
flattening.

.. image:: https://github.com/dmlc/web-data/raw/main/tvm/tutorial/array-packing.png
     :align: center


.. GENERATED FROM PYTHON SOURCE LINES 240-246

Just as it is shown in the figure above, after blocking the computations, we can observe the array
access pattern of B (after flattening), which is regular but discontinuous. We expect that after
some transformation we can get continuous access pattern. We can reorder a [16][16] array to
a [16/4][16][4] array, so that the access pattern of B will be sequential when grabing
the corresponding value from the packed array.


.. GENERATED FROM PYTHON SOURCE LINES 246-278

.. code-block:: default


    # We have to re-write the algorithm slightly.
    packedB = te.compute((N / bn, K, bn), lambda x, y, z: B[y, x * bn + z], name="packedB")
    C = te.compute(
        (M, N),
        lambda x, y: te.sum(A[x, k] * packedB[y // bn, k, tvm.tir.indexmod(y, bn)], axis=k),
        name="C",
    )

    s = te.create_schedule(C.op)

    xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)
    (k,) = s[C].op.reduce_axis
    ko, ki = s[C].split(k, factor=4)

    s[C].reorder(xo, yo, ko, xi, ki, yi)
    s[C].vectorize(yi)

    x, y, z = s[packedB].op.axis
    s[packedB].vectorize(z)
    s[packedB].parallel(x)

    func = tvm.build(s, [A, B, C], target=target, name="mmult")
    assert func

    c = tvm.nd.array(numpy.zeros((M, N), dtype=dtype), ctx)
    func(a, b, c)
    tvm.testing.assert_allclose(c.asnumpy(), answer, rtol=1e-5)

    evaluator = func.time_evaluator(func.entry_name, ctx, number=10)
    print("Opt4: %f" % evaluator(a, b, c).mean)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Opt4: 0.084636




.. GENERATED FROM PYTHON SOURCE LINES 279-280

Here is the generated IR after array packing.

.. GENERATED FROM PYTHON SOURCE LINES 280-283

.. code-block:: default


    print(tvm.lower(s, [A, B, C], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(A_1: handle, B_1: handle, C_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], []),
                 A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),
                 B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], [])}
      buffer_map = {A_1: A, B_1: B, C_1: C} {
      attr [packedB: Pointer(float32)] "storage_scope" = "global";
      allocate(packedB, float32x32, [32768]) {
        for (x: int32, 0, 32) "parallel" {
          for (y: int32, 0, 1024) {
            packedB[ramp(((x*32768) + (y*32)), 1, 32)] = (float32x32*)B_2[ramp(((y*1024) + (x*32)), 1, 32)]
          }
        }
        for (x.outer: int32, 0, 32) {
          for (y.outer: int32, 0, 32) {
            for (x.inner.init: int32, 0, 32) {
              C_2[ramp((((x.outer*32768) + (x.inner.init*1024)) + (y.outer*32)), 1, 32)] = broadcast(0f32, 32)
            }
            for (k.outer: int32, 0, 256) {
              for (x.inner: int32, 0, 32) {
                for (k.inner: int32, 0, 4) {
                  C_2[ramp((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)), 1, 32)] = ((float32x32*)C_2[ramp((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.inner*1024)) + (k.outer*4)) + k.inner)], 32)*(float32x32*)packedB[ramp((((y.outer*32768) + (k.outer*128)) + (k.inner*32)), 1, 32)]))
                }
              }
            }
          }
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 284-290

Write cache for blocks
----------------------
After blocking, the program will write result to C block by block, the access pattern
is not sequential. So we can use a sequential cache array to hold the block results and
write to C when all the block results are ready.


.. GENERATED FROM PYTHON SOURCE LINES 290-324

.. code-block:: default


    s = te.create_schedule(C.op)

    # Allocate write cache
    CC = s.cache_write(C, "global")

    xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)

    # Write cache is computed at yo
    s[CC].compute_at(s[C], yo)

    # New inner axes
    xc, yc = s[CC].op.axis

    (k,) = s[CC].op.reduce_axis
    ko, ki = s[CC].split(k, factor=4)
    s[CC].reorder(ko, xc, ki, yc)
    s[CC].unroll(ki)
    s[CC].vectorize(yc)

    x, y, z = s[packedB].op.axis
    s[packedB].vectorize(z)
    s[packedB].parallel(x)

    func = tvm.build(s, [A, B, C], target=target, name="mmult")
    assert func

    c = tvm.nd.array(numpy.zeros((M, N), dtype=dtype), ctx)
    func(a, b, c)
    tvm.testing.assert_allclose(c.asnumpy(), answer, rtol=1e-5)

    evaluator = func.time_evaluator(func.entry_name, ctx, number=10)
    print("Opt5: %f" % evaluator(a, b, c).mean)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Opt5: 0.078352




.. GENERATED FROM PYTHON SOURCE LINES 325-326

Here is the generated IR after blocking.

.. GENERATED FROM PYTHON SOURCE LINES 326-329

.. code-block:: default


    print(tvm.lower(s, [A, B, C], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(A_1: handle, B_1: handle, C_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),
                 C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], []),
                 B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], [])}
      buffer_map = {A_1: A, B_1: B, C_1: C} {
      attr [packedB: Pointer(float32)] "storage_scope" = "global";
      allocate(packedB, float32x32, [32768]);
      attr [C.global: Pointer(float32)] "storage_scope" = "global";
      allocate(C.global, float32, [1024]) {
        for (x: int32, 0, 32) "parallel" {
          for (y: int32, 0, 1024) {
            packedB[ramp(((x*32768) + (y*32)), 1, 32)] = (float32x32*)B_2[ramp(((y*1024) + (x*32)), 1, 32)]
          }
        }
        for (x.outer: int32, 0, 32) {
          for (y.outer: int32, 0, 32) {
            for (x.c.init: int32, 0, 32) {
              C.global[ramp((x.c.init*32), 1, 32)] = broadcast(0f32, 32)
            }
            for (k.outer: int32, 0, 256) {
              for (x.c: int32, 0, 32) {
                C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[(((x.outer*32768) + (x.c*1024)) + (k.outer*4))], 32)*(float32x32*)packedB[ramp(((y.outer*32768) + (k.outer*128)), 1, 32)]))
                C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.c*1024)) + (k.outer*4)) + 1)], 32)*(float32x32*)packedB[ramp((((y.outer*32768) + (k.outer*128)) + 32), 1, 32)]))
                C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.c*1024)) + (k.outer*4)) + 2)], 32)*(float32x32*)packedB[ramp((((y.outer*32768) + (k.outer*128)) + 64), 1, 32)]))
                C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.c*1024)) + (k.outer*4)) + 3)], 32)*(float32x32*)packedB[ramp((((y.outer*32768) + (k.outer*128)) + 96), 1, 32)]))
              }
            }
            for (x.inner: int32, 0, 32) {
              for (y.inner: int32, 0, 32) {
                C_2[((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)) + y.inner)] = (float32*)C.global[((x.inner*32) + y.inner)]
              }
            }
          }
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 330-333

Parallel
--------
Futhermore, we can also utilize multi-core processors to do the thread-level parallelization.

.. GENERATED FROM PYTHON SOURCE LINES 333-368

.. code-block:: default


    s = te.create_schedule(C.op)

    CC = s.cache_write(C, "global")

    xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)

    s[CC].compute_at(s[C], yo)

    xc, yc = s[CC].op.axis

    (k,) = s[CC].op.reduce_axis
    ko, ki = s[CC].split(k, factor=4)
    s[CC].reorder(ko, xc, ki, yc)
    s[CC].unroll(ki)
    s[CC].vectorize(yc)

    # parallel
    s[C].parallel(xo)

    x, y, z = s[packedB].op.axis
    s[packedB].vectorize(z)
    s[packedB].parallel(x)

    func = tvm.build(s, [A, B, C], target=target, name="mmult")
    assert func

    c = tvm.nd.array(numpy.zeros((M, N), dtype=dtype), ctx)
    func(a, b, c)
    tvm.testing.assert_allclose(c.asnumpy(), answer, rtol=1e-5)

    evaluator = func.time_evaluator(func.entry_name, ctx, number=50)
    opt6_time = evaluator(a, b, c).mean
    print("Opt6: %f" % opt6_time)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Opt6: 0.021094




.. GENERATED FROM PYTHON SOURCE LINES 369-370

Here is the generated IR after parallelization.

.. GENERATED FROM PYTHON SOURCE LINES 370-373

.. code-block:: default


    print(tvm.lower(s, [A, B, C], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(A_1: handle, B_1: handle, C_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], []),
                 A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),
                 B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], [])}
      buffer_map = {A_1: A, B_1: B, C_1: C} {
      attr [packedB: Pointer(float32)] "storage_scope" = "global";
      allocate(packedB, float32x32, [32768]) {
        for (x: int32, 0, 32) "parallel" {
          for (y: int32, 0, 1024) {
            packedB[ramp(((x*32768) + (y*32)), 1, 32)] = (float32x32*)B_2[ramp(((y*1024) + (x*32)), 1, 32)]
          }
        }
        for (x.outer: int32, 0, 32) "parallel" {
          attr [C.global: Pointer(float32)] "storage_scope" = "global";
          allocate(C.global, float32, [1024]);
          for (y.outer: int32, 0, 32) {
            for (x.c.init: int32, 0, 32) {
              C.global[ramp((x.c.init*32), 1, 32)] = broadcast(0f32, 32)
            }
            for (k.outer: int32, 0, 256) {
              for (x.c: int32, 0, 32) {
                C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[(((x.outer*32768) + (x.c*1024)) + (k.outer*4))], 32)*(float32x32*)packedB[ramp(((y.outer*32768) + (k.outer*128)), 1, 32)]))
                C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.c*1024)) + (k.outer*4)) + 1)], 32)*(float32x32*)packedB[ramp((((y.outer*32768) + (k.outer*128)) + 32), 1, 32)]))
                C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.c*1024)) + (k.outer*4)) + 2)], 32)*(float32x32*)packedB[ramp((((y.outer*32768) + (k.outer*128)) + 64), 1, 32)]))
                C.global[ramp((x.c*32), 1, 32)] = ((float32x32*)C.global[ramp((x.c*32), 1, 32)] + (broadcast((float32*)A_2[((((x.outer*32768) + (x.c*1024)) + (k.outer*4)) + 3)], 32)*(float32x32*)packedB[ramp((((y.outer*32768) + (k.outer*128)) + 96), 1, 32)]))
              }
            }
            for (x.inner: int32, 0, 32) {
              for (y.inner: int32, 0, 32) {
                C_2[((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)) + y.inner)] = (float32*)C.global[((x.inner*32) + y.inner)]
              }
            }
          }
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 376-383

Summary
-------
After applying the above simple optimizations with only 18 lines of code,
our generated code can achieve 60% of the `numpy` performance with MKL.
Note that the outputs on the web page reflect the running times on a non-exclusive
Docker container, thereby they are *unreliable*. It is highly encouraged to run the
tutorial by yourself to observe the performance gain acheived by TVM.


.. _sphx_glr_download_tutorials_optimize_opt_gemm.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: opt_gemm.py <opt_gemm.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: opt_gemm.ipynb <opt_gemm.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
