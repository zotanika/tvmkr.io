
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/language/scan.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorials_language_scan.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_language_scan.py:


Scan and Recurrent Kernel
=========================
**Author**: `Tianqi Chen <https://tqchen.github.io>`_

This is an introduction material on how to do recurrent computing in TVM.
Recurrent computing is a typical pattern in neural networks.

.. GENERATED FROM PYTHON SOURCE LINES 25-32

.. code-block:: default

    from __future__ import absolute_import, print_function

    import tvm
    import tvm.testing
    from tvm import te
    import numpy as np








.. GENERATED FROM PYTHON SOURCE LINES 33-51

TVM supports a scan operator to describe symbolic loop.
The following scan op computes cumsum over columns of X.

The scan is carried over the highest dimension of the tensor.
:code:`s_state` is a placeholder that describes the transition state of the scan.
:code:`s_init` describes how we can initialize the first k timesteps.
Here since s_init's first dimension is 1, it describes how we initialize
The state at first timestep.

:code:`s_update` describes how to update the value at timestep t. The update
value can refer back to the values of previous timestep via state placeholder.
Note that while it is invalid to refer to :code:`s_state` at current or later timestep.

The scan takes in state placeholder, initial value and update description.
It is also recommended(although not necessary) to list the inputs to the scan cell.
The result of the scan is a tensor, giving the result of :code:`s_state` after the
update over the time domain.


.. GENERATED FROM PYTHON SOURCE LINES 51-59

.. code-block:: default

    m = te.var("m")
    n = te.var("n")
    X = te.placeholder((m, n), name="X")
    s_state = te.placeholder((m, n))
    s_init = te.compute((1, n), lambda _, i: X[0, i])
    s_update = te.compute((m, n), lambda t, i: s_state[t - 1, i] + X[t, i])
    s_scan = tvm.te.scan(s_init, s_update, s_state, inputs=[X])








.. GENERATED FROM PYTHON SOURCE LINES 60-67

Schedule the Scan Cell
----------------------
We can schedule the body of the scan by scheduling the update and
init part seperately. Note that it is invalid to schedule the
first iteration dimension of the update part.
To split on the time iteration, user can schedule on scan_op.scan_axis instead.


.. GENERATED FROM PYTHON SOURCE LINES 67-79

.. code-block:: default

    s = te.create_schedule(s_scan.op)
    num_thread = 256
    block_x = te.thread_axis("blockIdx.x")
    thread_x = te.thread_axis("threadIdx.x")
    xo, xi = s[s_init].split(s_init.op.axis[1], factor=num_thread)
    s[s_init].bind(xo, block_x)
    s[s_init].bind(xi, thread_x)
    xo, xi = s[s_update].split(s_update.op.axis[1], factor=num_thread)
    s[s_update].bind(xo, block_x)
    s[s_update].bind(xi, thread_x)
    print(tvm.lower(s, [X, s_scan], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(X_1: handle, scan_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {scan: Buffer(scan_2: Pointer(float32), float32, [m: int32, n: int32], [stride: int32, stride_1: int32], type="auto"),
                 X: Buffer(X_2: Pointer(float32), float32, [m, n], [stride_2: int32, stride_3: int32], type="auto")}
      buffer_map = {X_1: X, scan_1: scan} {
      attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((n + 255), 256);
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 256;
      if @tir.likely((((blockIdx.x*256) + threadIdx.x) < n), dtype=bool) {
        scan_2[(((blockIdx.x*256) + threadIdx.x)*stride_1)] = (float32*)X_2[(((blockIdx.x*256) + threadIdx.x)*stride_3)]
      }
      for (scan.idx: int32, 0, (m - 1)) {
        attr [IterVar(blockIdx.x, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((n + 255), 256);
        attr [IterVar(threadIdx.x, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 256;
        if @tir.likely((((blockIdx.x*256) + threadIdx.x) < n), dtype=bool) {
          scan_2[(((scan.idx + 1)*stride) + (((blockIdx.x*256) + threadIdx.x)*stride_1))] = ((float32*)scan_2[((scan.idx*stride) + (((blockIdx.x*256) + threadIdx.x)*stride_1))] + (float32*)X_2[(((scan.idx + 1)*stride_2) + (((blockIdx.x*256) + threadIdx.x)*stride_3))])
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 80-85

Build and Verify
----------------
We can build the scan kernel like other TVM kernels, here we use
numpy to verify the correctness of the result.


.. GENERATED FROM PYTHON SOURCE LINES 85-95

.. code-block:: default

    fscan = tvm.build(s, [X, s_scan], "cuda", name="myscan")
    ctx = tvm.gpu(0)
    n = 1024
    m = 10
    a_np = np.random.uniform(size=(m, n)).astype(s_scan.dtype)
    a = tvm.nd.array(a_np, ctx)
    b = tvm.nd.array(np.zeros((m, n), dtype=s_scan.dtype), ctx)
    fscan(a, b)
    tvm.testing.assert_allclose(b.asnumpy(), np.cumsum(a_np, axis=0))



.. rst-class:: sphx-glr-script-out

.. code-block:: pytb

    Traceback (most recent call last):
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/tutorials/language/scan.py", line 85, in <module>
        fscan = tvm.build(s, [X, s_scan], "cuda", name="myscan")
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/driver/build_module.py", line 416, in build
        mod_host, mdev = _build_for_device(input_mod, tar, target_host)
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/driver/build_module.py", line 297, in _build_for_device
        rt_mod_dev = codegen.build_module(mod_dev, target) if len(mod_dev.functions) != 0 else None
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/target/codegen.py", line 39, in build_module
        return _ffi_api.Build(mod, target)
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
        raise get_last_ffi_error()
    tvm._ffi.base.TVMError: Traceback (most recent call last):
      [bt] (3) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(TVMFuncCall+0x65) [0x7f14f98ff845]
      [bt] (2) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target))::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0x677) [0x7f14f9299617]
      [bt] (1) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(tvm::codegen::Build(tvm::IRModule, tvm::Target)+0xe62) [0x7f14f9292f92]
      [bt] (0) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(+0xba1e42) [0x7f14f9291e42]
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/src/target/codegen.cc", line 58
    TVMError: 
    ---------------------------------------------------------------
    An internal invariant was violated during the execution of TVM.
    Please read TVM's error reporting guidelines.
    More details can be found here: https://discuss.tvm.ai/t/error-reporting/7793.
    ---------------------------------------------------------------
      Check failed: bf != nullptr == false: target.build.cuda is not enabled




.. GENERATED FROM PYTHON SOURCE LINES 96-105

Multi-Stage Scan Cell
---------------------
In the above example we described the scan cell using one Tensor
computation stage in s_update. It is possible to use multiple
Tensor stages in the scan cell.

The following lines demonstrate a scan with two stage operations
in the scan cell.


.. GENERATED FROM PYTHON SOURCE LINES 105-114

.. code-block:: default

    m = te.var("m")
    n = te.var("n")
    X = te.placeholder((m, n), name="X")
    s_state = te.placeholder((m, n))
    s_init = te.compute((1, n), lambda _, i: X[0, i])
    s_update_s1 = te.compute((m, n), lambda t, i: s_state[t - 1, i] * 2, name="s1")
    s_update_s2 = te.compute((m, n), lambda t, i: s_update_s1[t, i] + X[t, i], name="s2")
    s_scan = tvm.te.scan(s_init, s_update_s2, s_state, inputs=[X])


.. GENERATED FROM PYTHON SOURCE LINES 115-119

These intermediate tensors can also be scheduled normally.
To ensure correctness, TVM creates a group constraint to forbid
the body of scan to be compute_at locations outside the scan loop.


.. GENERATED FROM PYTHON SOURCE LINES 119-124

.. code-block:: default

    s = te.create_schedule(s_scan.op)
    xo, xi = s[s_update_s2].split(s_update_s2.op.axis[1], factor=32)
    s[s_update_s1].compute_at(s[s_update_s2], xo)
    print(tvm.lower(s, [X, s_scan], simple_mode=True))


.. GENERATED FROM PYTHON SOURCE LINES 125-131

Multiple States
---------------
For complicated applications like RNN, we might need more than one
recurrent state. Scan support multiple recurrent states.
The following example demonstrates how we can build recurrence with two states.


.. GENERATED FROM PYTHON SOURCE LINES 131-147

.. code-block:: default

    m = te.var("m")
    n = te.var("n")
    l = te.var("l")
    X = te.placeholder((m, n), name="X")
    s_state1 = te.placeholder((m, n))
    s_state2 = te.placeholder((m, l))
    s_init1 = te.compute((1, n), lambda _, i: X[0, i])
    s_init2 = te.compute((1, l), lambda _, i: 0.0)
    s_update1 = te.compute((m, n), lambda t, i: s_state1[t - 1, i] + X[t, i])
    s_update2 = te.compute((m, l), lambda t, i: s_state2[t - 1, i] + s_state1[t - 1, 0])
    s_scan1, s_scan2 = tvm.te.scan(
        [s_init1, s_init2], [s_update1, s_update2], [s_state1, s_state2], inputs=[X]
    )
    s = te.create_schedule(s_scan1.op)
    print(tvm.lower(s, [X, s_scan1, s_scan2], simple_mode=True))


.. GENERATED FROM PYTHON SOURCE LINES 148-155

Summary
-------
This tutorial provides a walk through of scan primitive.

- Describe scan with init and update.
- Schedule the scan cells as normal schedule.
- For complicated workload, use multiple states and steps in scan cell.


.. _sphx_glr_download_tutorials_language_scan.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: scan.py <scan.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: scan.ipynb <scan.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
