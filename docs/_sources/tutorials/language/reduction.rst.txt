
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/language/reduction.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorials_language_reduction.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_language_reduction.py:


Reduction
=========
**Author**: `Tianqi Chen <https://tqchen.github.io>`_

This is an introduction material on how to do reduction in TVM.
Associative reduction operators like sum/max/min are typical
construction blocks of linear algebra operations.

In this tutorial, we will demonstrate how to do reduction in TVM.

.. GENERATED FROM PYTHON SOURCE LINES 28-35

.. code-block:: default

    from __future__ import absolute_import, print_function

    import tvm
    import tvm.testing
    from tvm import te
    import numpy as np








.. GENERATED FROM PYTHON SOURCE LINES 36-58

Describe Sum of Rows
--------------------
Assume we want to compute sum of rows as our example.
In numpy semantics this can be written as :code:`B = numpy.sum(A, axis=1)`

The following lines describe the row sum operation.
To create a reduction formula, we declare a reduction axis using
:any:`te.reduce_axis`. :any:`te.reduce_axis` takes in the range of reductions.
:any:`te.sum` takes in the expression to be reduced as well as the reduction
axis and compute the sum of value over all k in the declared range.

The equivalent C code is as follows:

.. code-block:: c

  for (int i = 0; i < n; ++i) {
    B[i] = 0;
    for (int k = 0; k < m; ++k) {
      B[i] = B[i] + A[i][k];
    }
  }


.. GENERATED FROM PYTHON SOURCE LINES 58-64

.. code-block:: default

    n = te.var("n")
    m = te.var("m")
    A = te.placeholder((n, m), name="A")
    k = te.reduce_axis((0, m), "k")
    B = te.compute((n,), lambda i: te.sum(A[i, k], axis=k), name="B")








.. GENERATED FROM PYTHON SOURCE LINES 65-70

Schedule the Reduction
----------------------
There are several ways to schedule a reduction.
Before doing anything, let us print out the IR code of default schedule.


.. GENERATED FROM PYTHON SOURCE LINES 70-73

.. code-block:: default

    s = te.create_schedule(B.op)
    print(tvm.lower(s, [A, B], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(A_1: handle, B_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {B: Buffer(B_2: Pointer(float32), float32, [n: int32], [stride: int32], type="auto"),
                 A: Buffer(A_2: Pointer(float32), float32, [n, m: int32], [stride_1: int32, stride_2: int32], type="auto")}
      buffer_map = {A_1: A, B_1: B} {
      for (i: int32, 0, n) {
        B_2[(i*stride)] = 0f32
        for (k: int32, 0, m) {
          B_2[(i*stride)] = ((float32*)B_2[(i*stride)] + (float32*)A_2[((i*stride_1) + (k*stride_2))])
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 74-80

You can find that the IR code is quite like the C code.
The reduction axis is similar to a normal axis, it can be splitted.

In the following code we split both the row axis of B as well
axis by different factors. The result is a nested reduction.


.. GENERATED FROM PYTHON SOURCE LINES 80-84

.. code-block:: default

    ko, ki = s[B].split(B.op.reduce_axis[0], factor=16)
    xo, xi = s[B].split(B.op.axis[0], factor=32)
    print(tvm.lower(s, [A, B], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(A_1: handle, B_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {B: Buffer(B_2: Pointer(float32), float32, [n: int32], [stride: int32], type="auto"),
                 A: Buffer(A_2: Pointer(float32), float32, [n, m: int32], [stride_1: int32, stride_2: int32], type="auto")}
      buffer_map = {A_1: A, B_1: B} {
      for (i.outer: int32, 0, floordiv((n + 31), 32)) {
        for (i.inner: int32, 0, 32) {
          if @tir.likely((((i.outer*32) + i.inner) < n), dtype=bool) {
            B_2[(((i.outer*32) + i.inner)*stride)] = 0f32
          }
          if @tir.likely((((i.outer*32) + i.inner) < n), dtype=bool) {
            for (k.outer: int32, 0, floordiv((m + 15), 16)) {
              for (k.inner: int32, 0, 16) {
                if @tir.likely((((k.outer*16) + k.inner) < m), dtype=bool) {
                  B_2[(((i.outer*32) + i.inner)*stride)] = ((float32*)B_2[(((i.outer*32) + i.inner)*stride)] + (float32*)A_2[((((i.outer*32) + i.inner)*stride_1) + (((k.outer*16) + k.inner)*stride_2))])
                }
              }
            }
          }
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 85-86

If we are building a GPU kernel, we can bind the rows of B to GPU threads.

.. GENERATED FROM PYTHON SOURCE LINES 86-90

.. code-block:: default

    s[B].bind(xo, te.thread_axis("blockIdx.x"))
    s[B].bind(xi, te.thread_axis("threadIdx.x"))
    print(tvm.lower(s, [A, B], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(A_1: handle, B_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {B: Buffer(B_2: Pointer(float32), float32, [n: int32], [stride: int32], type="auto"),
                 A: Buffer(A_2: Pointer(float32), float32, [n, m: int32], [stride_1: int32, stride_2: int32], type="auto")}
      buffer_map = {A_1: A, B_1: B} {
      attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = floordiv((n + 31), 32);
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32 {
        if @tir.likely((((blockIdx.x*32) + threadIdx.x) < n), dtype=bool) {
          B_2[(((blockIdx.x*32) + threadIdx.x)*stride)] = 0f32
        }
        for (k.outer: int32, 0, floordiv((m + 15), 16)) {
          for (k.inner: int32, 0, 16) {
            if @tir.likely((((blockIdx.x*32) + threadIdx.x) < n), dtype=bool) {
              if @tir.likely((((k.outer*16) + k.inner) < m), dtype=bool) {
                B_2[(((blockIdx.x*32) + threadIdx.x)*stride)] = ((float32*)B_2[(((blockIdx.x*32) + threadIdx.x)*stride)] + (float32*)A_2[((((blockIdx.x*32) + threadIdx.x)*stride_1) + (((k.outer*16) + k.inner)*stride_2))])
              }
            }
          }
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 91-102

Reduction Factoring and Parallelization
---------------------------------------
One problem of building a reduction is that we cannot simply
parallelize over the reduction axis. We need to divide the computation
of the reduction, store the local reduction result in a temporal array
before doing a reduction over the temp array.

The rfactor primitive does such rewrite of the computation.
In the following schedule, the result of B is written to a temporary
result B.rf. The factored dimension becomes the first dimension of B.rf.


.. GENERATED FROM PYTHON SOURCE LINES 102-107

.. code-block:: default

    s = te.create_schedule(B.op)
    ko, ki = s[B].split(B.op.reduce_axis[0], factor=16)
    BF = s.rfactor(B, ki)
    print(tvm.lower(s, [A, B], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(A_1: handle, B_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {B: Buffer(B_2: Pointer(float32), float32, [n: int32], [stride: int32], type="auto"),
                 A: Buffer(A_2: Pointer(float32), float32, [n, m: int32], [stride_1: int32, stride_2: int32], type="auto")}
      buffer_map = {A_1: A, B_1: B} {
      attr [B.rf: Pointer(float32)] "storage_scope" = "global";
      allocate(B.rf, float32, [(n*16)]) {
        for (k.inner: int32, 0, 16) {
          for (i: int32, 0, n) {
            B.rf[((k.inner*n) + i)] = 0f32
            for (k.outer: int32, 0, floordiv((m + 15), 16)) {
              if @tir.likely((((k.outer*16) + k.inner) < m), dtype=bool) {
                B.rf[((k.inner*n) + i)] = ((float32*)B.rf[((k.inner*n) + i)] + (float32*)A_2[((i*stride_1) + (((k.outer*16) + k.inner)*stride_2))])
              }
            }
          }
        }
        for (ax0: int32, 0, n) {
          B_2[(ax0*stride)] = 0f32
          for (k.inner.v: int32, 0, 16) {
            B_2[(ax0*stride)] = ((float32*)B_2[(ax0*stride)] + (float32*)B.rf[((k.inner.v*n) + ax0)])
          }
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 108-111

The scheduled operator of B also get rewritten to be sum over
the first axis of reduced result of B.f


.. GENERATED FROM PYTHON SOURCE LINES 111-113

.. code-block:: default

    print(s[B].op.body)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[B.rf[k.inner.v, ax0]], init=[], axis=[iter_var(k.inner.v, range(min=0, ext=16))], where=(bool)1, value_index=0)]




.. GENERATED FROM PYTHON SOURCE LINES 114-126

Cross Thread Reduction
----------------------
We can now parallelize over the factored axis.
Here the reduction axis of B is marked to be a thread.
TVM allows reduction axis to be marked as thread if it is the only
axis in reduction and cross thread reduction is possible in the device.

This is indeed the case after the factoring.
We can directly compute BF at the reduction axis as well.
The final generated kernel will divide the rows by blockIdx.x and threadIdx.y
columns by threadIdx.x and finally do a cross thread reduction over threadIdx.x


.. GENERATED FROM PYTHON SOURCE LINES 126-136

.. code-block:: default

    xo, xi = s[B].split(s[B].op.axis[0], factor=32)
    s[B].bind(xo, te.thread_axis("blockIdx.x"))
    s[B].bind(xi, te.thread_axis("threadIdx.y"))
    tx = te.thread_axis("threadIdx.x")
    s[B].bind(s[B].op.reduce_axis[0], tx)
    s[BF].compute_at(s[B], s[B].op.reduce_axis[0])
    s[B].set_store_predicate(tx.var.equal(0))
    fcuda = tvm.build(s, [A, B], "cuda")
    print(fcuda.imported_modules[0].get_source())



.. rst-class:: sphx-glr-script-out

.. code-block:: pytb

    Traceback (most recent call last):
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/tutorials/language/reduction.py", line 133, in <module>
        fcuda = tvm.build(s, [A, B], "cuda")
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/driver/build_module.py", line 416, in build
        mod_host, mdev = _build_for_device(input_mod, tar, target_host)
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/driver/build_module.py", line 297, in _build_for_device
        rt_mod_dev = codegen.build_module(mod_dev, target) if len(mod_dev.functions) != 0 else None
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/target/codegen.py", line 39, in build_module
        return _ffi_api.Build(mod, target)
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
        raise get_last_ffi_error()
    tvm._ffi.base.TVMError: Traceback (most recent call last):
      [bt] (3) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(TVMFuncCall+0x65) [0x7f14f98ff845]
      [bt] (2) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target))::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0x677) [0x7f14f9299617]
      [bt] (1) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(tvm::codegen::Build(tvm::IRModule, tvm::Target)+0xe62) [0x7f14f9292f92]
      [bt] (0) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(+0xba1e42) [0x7f14f9291e42]
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/src/target/codegen.cc", line 58
    TVMError: 
    ---------------------------------------------------------------
    An internal invariant was violated during the execution of TVM.
    Please read TVM's error reporting guidelines.
    More details can be found here: https://discuss.tvm.ai/t/error-reporting/7793.
    ---------------------------------------------------------------
      Check failed: bf != nullptr == false: target.build.cuda is not enabled




.. GENERATED FROM PYTHON SOURCE LINES 137-139

Verify the correctness of result kernel by comparing it to numpy.


.. GENERATED FROM PYTHON SOURCE LINES 139-146

.. code-block:: default

    nn = 128
    ctx = tvm.gpu(0)
    a = tvm.nd.array(np.random.uniform(size=(nn, nn)).astype(A.dtype), ctx)
    b = tvm.nd.array(np.zeros(nn, dtype=B.dtype), ctx)
    fcuda(a, b)
    tvm.testing.assert_allclose(b.asnumpy(), np.sum(a.asnumpy(), axis=1), rtol=1e-4)


.. GENERATED FROM PYTHON SOURCE LINES 147-152

Describe Convolution via 2D Reduction
-------------------------------------
In TVM, we can describe convolution via 2D reduction in a simple way.
Here is an example for 2D convolution with filter size = [3, 3] and strides = [1, 1].


.. GENERATED FROM PYTHON SOURCE LINES 152-165

.. code-block:: default

    n = te.var("n")
    Input = te.placeholder((n, n), name="Input")
    Filter = te.placeholder((3, 3), name="Filter")
    di = te.reduce_axis((0, 3), name="di")
    dj = te.reduce_axis((0, 3), name="dj")
    Output = te.compute(
        (n - 2, n - 2),
        lambda i, j: te.sum(Input[i + di, j + dj] * Filter[di, dj], axis=[di, dj]),
        name="Output",
    )
    s = te.create_schedule(Output.op)
    print(tvm.lower(s, [Input, Filter, Output], simple_mode=True))


.. GENERATED FROM PYTHON SOURCE LINES 166-174

.. _general-reduction:

Define General Commutative Reduction Operation
----------------------------------------------
Besides the built-in reduction operations like :any:`te.sum`,
:any:`tvm.te.min` and :any:`tvm.te.max`, you can also define your
commutative reduction operation by :any:`te.comm_reducer`.


.. GENERATED FROM PYTHON SOURCE LINES 174-182

.. code-block:: default


    n = te.var("n")
    m = te.var("m")
    product = te.comm_reducer(lambda x, y: x * y, lambda t: tvm.tir.const(1, dtype=t), name="product")
    A = te.placeholder((n, m), name="A")
    k = te.reduce_axis((0, m), name="k")
    B = te.compute((n,), lambda i: product(A[i, k], axis=k), name="B")


.. GENERATED FROM PYTHON SOURCE LINES 183-188

.. note::

  Sometimes we would like to perform reduction that involves multiple
  values like :code:`argmax`, which can be done by tuple inputs.
  See :ref:`reduction-with-tuple-inputs` for more detail.

.. GENERATED FROM PYTHON SOURCE LINES 190-197

Summary
-------
This tutorial provides a walk through of reduction schedule.

- Describe reduction with reduce_axis.
- Use rfactor to factor out axis if we need parallelism.
- Define new reduction operation by :any:`te.comm_reducer`


.. _sphx_glr_download_tutorials_language_reduction.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: reduction.py <reduction.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: reduction.ipynb <reduction.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
