
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/topi/intro_topi.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorials_topi_intro_topi.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_topi_intro_topi.py:


Introduction to TOPI
====================
**Author**: `Ehsan M. Kermani <https://github.com/ehsanmok>`_

This is an introductory tutorial to TVM Operator Inventory (TOPI).
TOPI provides numpy-style generic operations and schedules with higher abstractions than TVM.
In this tutorial, we will see how TOPI can save us from writing boilerplates code in TVM.

.. GENERATED FROM PYTHON SOURCE LINES 26-34

.. code-block:: default

    from __future__ import absolute_import, print_function

    import tvm
    import tvm.testing
    from tvm import te
    from tvm import topi
    import numpy as np








.. GENERATED FROM PYTHON SOURCE LINES 35-41

Basic example
-------------
Let's revisit the sum of rows operation (equivalent to :code:`B = numpy.sum(A, axis=1)`') \
To compute the sum of rows of a two dimensional TVM tensor A, we should
specify the symbolic operation as well as schedule as follows


.. GENERATED FROM PYTHON SOURCE LINES 41-48

.. code-block:: default

    n = te.var("n")
    m = te.var("m")
    A = te.placeholder((n, m), name="A")
    k = te.reduce_axis((0, m), "k")
    B = te.compute((n,), lambda i: te.sum(A[i, k], axis=k), name="B")
    s = te.create_schedule(B.op)








.. GENERATED FROM PYTHON SOURCE LINES 49-51

and to examine the IR code in human readable format, we can do


.. GENERATED FROM PYTHON SOURCE LINES 51-53

.. code-block:: default

    print(tvm.lower(s, [A], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(A_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {A: Buffer(A_2: Pointer(float32), float32, [n: int32, m: int32], [stride: int32, stride_1: int32], type="auto")}
      buffer_map = {A_1: A} {
      attr [B: Pointer(float32)] "storage_scope" = "global";
      allocate(B, float32, [n]);
      for (i: int32, 0, n) {
        B[i] = 0f32
        for (k: int32, 0, m) {
          B[i] = ((float32*)B[i] + (float32*)A_2[((i*stride) + (k*stride_1))])
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 54-58

However, for such a common operation we had to define the reduce axis ourselves as well as explicit computation with
:code:`te.compute`. Imagine for more complicated operations how much details we need to provide.
Fortunately, we can replace those two lines with simple :code:`topi.sum` much like :code:`numpy.sum`


.. GENERATED FROM PYTHON SOURCE LINES 58-62

.. code-block:: default

    C = topi.sum(A, axis=1)
    ts = te.create_schedule(C.op)
    print(tvm.lower(ts, [A], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(A_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {A: Buffer(A_2: Pointer(float32), float32, [n: int32, m: int32], [stride: int32, stride_1: int32], type="auto")}
      buffer_map = {A_1: A} {
      attr [A_red: Pointer(float32)] "storage_scope" = "global";
      allocate(A_red, float32, [n]);
      for (ax0: int32, 0, n) {
        A_red[ax0] = 0f32
        for (k1: int32, 0, m) {
          A_red[ax0] = ((float32*)A_red[ax0] + (float32*)A_2[((ax0*stride) + (k1*stride_1))])
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 63-68

Numpy-style operator overloading
--------------------------------
We can add two tensors using :code:`topi.broadcast_add` that have correct (broadcastable with specific) shapes.
Even shorter, TOPI provides operator overloading for such common operations. For example,


.. GENERATED FROM PYTHON SOURCE LINES 68-74

.. code-block:: default

    x, y = 100, 10
    a = te.placeholder((x, y, y), name="a")
    b = te.placeholder((y, y), name="b")
    c = a + b  # same as topi.broadcast_add
    d = a * b  # same as topi.broadcast_mul








.. GENERATED FROM PYTHON SOURCE LINES 75-76

Overloaded with the same syntax, TOPI handles broadcasting a primitive (`int`, `float`) to a tensor :code:`d - 3.14`.

.. GENERATED FROM PYTHON SOURCE LINES 78-86

Generic schedules and fusing operations
---------------------------------------
Up to now, we have seen an example of how TOPI can save us from writing explicit computations in lower level API.
But it doesn't stop here. Still we did the scheduling as before. TOPI also provides higher level
scheduling recipes depending on a given context. For example, for CUDA,
we can schedule the following series of operations ending with :code:`topi.sum` using only
:code:`topi.generic.schedule_reduce`


.. GENERATED FROM PYTHON SOURCE LINES 86-93

.. code-block:: default

    e = topi.elemwise_sum([c, d])
    f = e / 2.0
    g = topi.sum(f)
    with tvm.target.cuda():
        sg = topi.cuda.schedule_reduce(g)
        print(tvm.lower(sg, [a, b], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(a_1: handle, b_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {b: Buffer(b_2: Pointer(float32), float32, [10, 10], []),
                 a: Buffer(a_2: Pointer(float32), float32, [100, 10, 10], [])}
      buffer_map = {a_1: a, b_1: b} {
      attr [T_divide_red: Pointer(float32)] "storage_scope" = "global";
      allocate(T_divide_red, float32, [1]);
      attr [IterVar(threadIdx.x: int32, [0:1024], "ThreadIndex", "threadIdx.x")] "thread_extent" = 1024;
      attr [T_divide_red.rf: Pointer(float32)] "storage_scope" = "local";
      allocate(T_divide_red.rf, float32, [1]);
      attr [reduce_temp0: handle] "storage_scope" = "local";
      allocate(reduce_temp0, float32, [1]) {
        T_divide_red.rf[0] = 0f32
        for (k0.k1.fused.k2.fused.outer: int32, 0, 10) {
          if @tir.likely((((((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x) < 10000) && (((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x) < 10000)) && (((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x) < 10000)), dtype=bool) {
            T_divide_red.rf[0] = ((float32*)T_divide_red.rf[0] + ((((float32*)a_2[((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x)] + (float32*)b_2[floormod(((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x), 100)]) + ((float32*)a_2[((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x)]*(float32*)b_2[floormod(((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x), 100)]))*0.5f32))
          }
        }
        attr [meta[tir.CommReducer][0]] "reduce_scope" = @tir.reinterpret(0u64, dtype=handle);
        @tir.tvm_thread_allreduce(1u32, (float32*)T_divide_red.rf[0], True, reduce_temp0, threadIdx.x, dtype=handle)
        if (threadIdx.x == 0) {
          T_divide_red[0] = (float32*)reduce_temp0[0]
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 94-96

As you can see, scheduled stages of computation have been accumulated and we can examine them by


.. GENERATED FROM PYTHON SOURCE LINES 96-98

.. code-block:: default

    print(sg.stages)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [stage(a, placeholder(a, 0xf507b10)), stage(b, placeholder(b, 0x90a2390)), stage(T_add, compute(T_add, body=[(a[ax0, ax1, ax2] + b[ax1, ax2])], axis=[iter_var(ax0, range(min=0, ext=100)), iter_var(ax1, range(min=0, ext=10)), iter_var(ax2, range(min=0, ext=10))], reduce_axis=[], tag=broadcast, attrs={})), stage(T_multiply, compute(T_multiply, body=[(a[ax0, ax1, ax2]*b[ax1, ax2])], axis=[iter_var(ax0, range(min=0, ext=100)), iter_var(ax1, range(min=0, ext=10)), iter_var(ax2, range(min=0, ext=10))], reduce_axis=[], tag=broadcast, attrs={})), stage(T_elemwise_sum, compute(T_elemwise_sum, body=[(T_add[ax0, ax1, ax2] + T_multiply[ax0, ax1, ax2])], axis=[iter_var(ax0, range(min=0, ext=100)), iter_var(ax1, range(min=0, ext=10)), iter_var(ax2, range(min=0, ext=10))], reduce_axis=[], tag=elemwise, attrs={})), stage(T_divide, compute(T_divide, body=[(T_elemwise_sum[ax0, ax1, ax2]/2f)], axis=[iter_var(ax0, range(min=0, ext=100)), iter_var(ax1, range(min=0, ext=10)), iter_var(ax2, range(min=0, ext=10))], reduce_axis=[], tag=elemwise, attrs={})), stage(T_divide_red.rf, compute(T_divide_red.rf, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_divide[floordiv(floordiv((k0.k1.fused.k2.fused.inner + (k0.k1.fused.k2.fused.outer*1024)), 10), 10), floormod(floordiv((k0.k1.fused.k2.fused.inner + (k0.k1.fused.k2.fused.outer*1024)), 10), 10), floormod((k0.k1.fused.k2.fused.inner + (k0.k1.fused.k2.fused.outer*1024)), 10)]], init=[], axis=[iter_var(k0.k1.fused.k2.fused.outer, range(min=0, ext=10))], where=tir.likely((((floordiv(floordiv((k0.k1.fused.k2.fused.inner + (k0.k1.fused.k2.fused.outer*1024)), 10), 10) < 100) && (floordiv((k0.k1.fused.k2.fused.inner + (k0.k1.fused.k2.fused.outer*1024)), 10) < 1000)) && ((k0.k1.fused.k2.fused.inner + (k0.k1.fused.k2.fused.outer*1024)) < 10000))), value_index=0)], axis=[iter_var(k0.k1.fused.k2.fused.inner, range(min=0, ext=1024))], reduce_axis=[iter_var(k0.k1.fused.k2.fused.outer, range(min=0, ext=10))], tag=, attrs={})), stage(T_divide_red, compute(T_divide_red.repl, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_divide_red.rf[k0.k1.fused.k2.fused.inner.v]], init=[], axis=[iter_var(k0.k1.fused.k2.fused.inner.v, range(min=0, ext=1024))], where=(bool)1, value_index=0)], axis=[], reduce_axis=[iter_var(k0.k1.fused.k2.fused.inner.v, range(min=0, ext=1024))], tag=, attrs={}))]




.. GENERATED FROM PYTHON SOURCE LINES 99-101

We can test the correctness by comparing with :code:`numpy` result as follows


.. GENERATED FROM PYTHON SOURCE LINES 101-112

.. code-block:: default

    func = tvm.build(sg, [a, b, g], "cuda")
    ctx = tvm.gpu(0)
    a_np = np.random.uniform(size=(x, y, y)).astype(a.dtype)
    b_np = np.random.uniform(size=(y, y)).astype(b.dtype)
    g_np = np.sum(np.add(a_np + b_np, a_np * b_np) / 2.0)
    a_nd = tvm.nd.array(a_np, ctx)
    b_nd = tvm.nd.array(b_np, ctx)
    g_nd = tvm.nd.array(np.zeros(g_np.shape, dtype=g_np.dtype), ctx)
    func(a_nd, b_nd, g_nd)
    tvm.testing.assert_allclose(g_nd.asnumpy(), g_np, rtol=1e-5)



.. rst-class:: sphx-glr-script-out

.. code-block:: pytb

    Traceback (most recent call last):
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/tutorials/topi/intro_topi.py", line 101, in <module>
        func = tvm.build(sg, [a, b, g], "cuda")
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/driver/build_module.py", line 416, in build
        mod_host, mdev = _build_for_device(input_mod, tar, target_host)
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/driver/build_module.py", line 297, in _build_for_device
        rt_mod_dev = codegen.build_module(mod_dev, target) if len(mod_dev.functions) != 0 else None
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/target/codegen.py", line 39, in build_module
        return _ffi_api.Build(mod, target)
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
        raise get_last_ffi_error()
    tvm._ffi.base.TVMError: Traceback (most recent call last):
      [bt] (3) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(TVMFuncCall+0x65) [0x7f14f98ff845]
      [bt] (2) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule, tvm::Target)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)>(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target))::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0x677) [0x7f14f9299617]
      [bt] (1) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(tvm::codegen::Build(tvm::IRModule, tvm::Target)+0xe62) [0x7f14f9292f92]
      [bt] (0) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(+0xba1e42) [0x7f14f9291e42]
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/src/target/codegen.cc", line 58
    TVMError: 
    ---------------------------------------------------------------
    An internal invariant was violated during the execution of TVM.
    Please read TVM's error reporting guidelines.
    More details can be found here: https://discuss.tvm.ai/t/error-reporting/7793.
    ---------------------------------------------------------------
      Check failed: bf != nullptr == false: target.build.cuda is not enabled




.. GENERATED FROM PYTHON SOURCE LINES 113-115

TOPI also provides common neural nets operations such as _softmax_ with optimized schedule


.. GENERATED FROM PYTHON SOURCE LINES 115-121

.. code-block:: default

    tarray = te.placeholder((512, 512), name="tarray")
    softmax_topi = topi.nn.softmax(tarray)
    with tvm.target.Target("cuda"):
        sst = topi.cuda.schedule_softmax(softmax_topi)
        print(tvm.lower(sst, [tarray], simple_mode=True))


.. GENERATED FROM PYTHON SOURCE LINES 122-133

Fusing convolutions
-------------------
We can fuse :code:`topi.nn.conv2d` and :code:`topi.nn.relu` together.

.. note::

   TOPI functions are all generic functions. They have different implementations
   for different backends to optimize for performance.
   For each backend, it is necessary to call them under a target scope for both
   compute declaration and schedule. TVM will choose the right function to call with
   the target information.

.. GENERATED FROM PYTHON SOURCE LINES 133-143

.. code-block:: default


    data = te.placeholder((1, 3, 224, 224))
    kernel = te.placeholder((10, 3, 5, 5))

    with tvm.target.Target("cuda"):
        conv = topi.cuda.conv2d_nchw(data, kernel, 1, 2, 1)
        out = topi.nn.relu(conv)
        sconv = topi.cuda.schedule_conv2d_nchw([out])
        print(tvm.lower(sconv, [data, kernel], simple_mode=True))


.. GENERATED FROM PYTHON SOURCE LINES 144-150

Summary
-------
In this tutorial, we have seen

- How to use TOPI API for common operations with numpy-style operators.
- How TOPI facilitates generic schedules and operator fusion for a context, to generate optimized kernel codes.


.. _sphx_glr_download_tutorials_topi_intro_topi.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: intro_topi.py <intro_topi.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: intro_topi.ipynb <intro_topi.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
