
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/auto_scheduler/tune_matmul_x86.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorials_auto_scheduler_tune_matmul_x86.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_auto_scheduler_tune_matmul_x86.py:


Auto-scheduling Matrix Multiplication for CPU
=============================================
**Author**: `Lianmin Zheng <https://github.com/merrymercy>`_,             `Chengfan Jia <https://github.com/jcf94/>`_

This is a tutorial on how to use the auto-scheduler for CPUs.

Different from the template-based :ref:`autotvm <tutorials-autotvm-sec>` which relies on
manual templates to define the search space, the auto-scheduler does not require any templates.
Users only need to write the computation declaration without any schedule commands or templates.
The auto-scheduler can automatically generate a large search space and
find a good schedule in the space.

We use matrix multiplication as an example in this tutorial.

Note that this tutorial will not run on Windows or recent versions of macOS. To
get it to run, you will need to wrap the body of this tutorial in a :code:`if
__name__ == "__main__":` block.

.. GENERATED FROM PYTHON SOURCE LINES 37-44

.. code-block:: default


    import os

    import numpy as np
    import tvm
    from tvm import te, auto_scheduler








.. GENERATED FROM PYTHON SOURCE LINES 45-50

Define the computation
^^^^^^^^^^^^^^^^^^^^^^
To begin with, let us define the computation of a matmul with bias add.
The function should return the list of input/output tensors.
From these tensors, the auto-scheduler can get the whole computational graph.

.. GENERATED FROM PYTHON SOURCE LINES 50-70

.. code-block:: default



    @auto_scheduler.register_workload
    def matmul_add(N, L, M, dtype):
        A = te.placeholder((N, L), name="A", dtype=dtype)
        B = te.placeholder((L, M), name="B", dtype=dtype)
        C = te.placeholder((N, M), name="C", dtype=dtype)

        k = te.reduce_axis((0, L), name="k")
        matmul = te.compute(
            (N, M),
            lambda i, j: te.sum(A[i, k] * B[k, j], axis=k),
            name="matmul",
            attrs={"layout_free_placeholders": [B]},  # enable automatic layout transform for tensor B
        )
        out = te.compute((N, M), lambda i, j: matmul[i, j] + C[i, j], name="out")

        return [A, B, C, out]









.. GENERATED FROM PYTHON SOURCE LINES 71-78

Create the search task
^^^^^^^^^^^^^^^^^^^^^^
We then create a search task with N=L=M=1024 and dtype="float32"
If your machine supports avx instructions, you can

  - replace "llvm" below with "llvm -mcpu=core-avx2" to enable AVX2
  - replace "llvm" below with "llvm -mcpu=skylake-avx512" to enable AVX-512

.. GENERATED FROM PYTHON SOURCE LINES 78-87

.. code-block:: default


    target = tvm.target.Target("llvm")
    N = L = M = 1024
    task = tvm.auto_scheduler.SearchTask(func=matmul_add, args=(N, L, M, "float32"), target=target)

    # Inspect the computational graph
    print("Computational DAG:")
    print(task.compute_dag)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Computational DAG:
    A = PLACEHOLDER [1024, 1024]
    B = PLACEHOLDER [1024, 1024]
    matmul(i, j) += (A[i, k]*B[k, j])
    C = PLACEHOLDER [1024, 1024]
    out(i, j) = (matmul[i, j] + C[i, j])





.. GENERATED FROM PYTHON SOURCE LINES 88-97

Next, we set parameters for the auto-scheduler.

* :code:`num_measure_trials` is the number of measurement trials we can use during the search.
  We only make 10 trials in this tutorial for a fast demonstration. In practice, 1000 is a
  good value for the search to converge. You can do more trials according to your time budget.
* In addition, we use :code:`RecordToFile` to dump measurement records into a file `matmul.json`.
  The measurement records can be used to query the history best, resume the search,
  and do more analyses later.
* see :any:`auto_scheduler.TuningOptions` for more parameters

.. GENERATED FROM PYTHON SOURCE LINES 97-105

.. code-block:: default


    log_file = "matmul.json"
    tune_option = auto_scheduler.TuningOptions(
        num_measure_trials=10,
        measure_callbacks=[auto_scheduler.RecordToFile(log_file)],
        verbose=2,
    )








.. GENERATED FROM PYTHON SOURCE LINES 106-112

Run the search
^^^^^^^^^^^^^^
Now we get all inputs ready. Pretty simple, isn't it?
We can kick off the search and let the auto-scheduler do its magic.
After some measurement trials, we can load the best schedule from the log
file and apply it.

.. GENERATED FROM PYTHON SOURCE LINES 112-118

.. code-block:: default


    # Run auto-tuning (search)
    task.tune(tune_option)
    # Apply the best schedule
    sch, args = task.apply_best(log_file)








.. GENERATED FROM PYTHON SOURCE LINES 119-122

We can lower the schedule to see the IR after auto-scheduling.
The auto-scheduler correctly performs optimizations including multi-level tiling,
layout transformation, parallelization, vectorization, unrolling, and operator fusion.

.. GENERATED FROM PYTHON SOURCE LINES 122-126

.. code-block:: default


    print("Lowered TIR:")
    print(tvm.lower(sch, args, simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Lowered TIR:
    primfn(A_1: handle, B_1: handle, C_1: handle, out_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {out: Buffer(out_2: Pointer(float32), float32, [1024, 1024], []),
                 C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], []),
                 A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),
                 B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], [])}
      buffer_map = {A_1: A, B_1: B, C_1: C, out_1: out} {
      attr [auto_scheduler_layout_transform: Pointer(float32)] "storage_scope" = "global";
      allocate(auto_scheduler_layout_transform, float32, [1048576]);
      attr [matmul: Pointer(float32)] "storage_scope" = "global";
      allocate(matmul, float32, [1048576]) {
        for (ax0.ax1.fused.ax2.fused: int32, 0, 16384) "parallel" {
          for (ax3: int32, 0, 4) {
            for (ax5: int32, 0, 16) {
              auto_scheduler_layout_transform[(((ax0.ax1.fused.ax2.fused*64) + (ax3*16)) + ax5)] = (float32*)B_2[((((floormod(ax0.ax1.fused.ax2.fused, 1024)*1024) + (floordiv(ax0.ax1.fused.ax2.fused, 1024)*64)) + (ax3*16)) + ax5)]
            }
          }
        }
        for (i.outer.outer.outer.j.outer.outer.outer.fused: int32, 0, 4096) "parallel" {
          for (i.outer.outer.inner: int32, 0, 4) {
            matmul[ramp((((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + (floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*64)), 1, 16)] = broadcast(0f32, 16)
            matmul[ramp(((((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + (floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*64)) + 16), 1, 16)] = broadcast(0f32, 16)
            matmul[ramp(((((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + (floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*64)) + 32), 1, 16)] = broadcast(0f32, 16)
            matmul[ramp(((((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + (floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*64)) + 48), 1, 16)] = broadcast(0f32, 16)
            for (k.outer: int32, 0, 1024) {
              matmul[ramp((((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + (floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*64)), 1, 16)] = ((float32x16*)matmul[ramp((((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + (floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*64)), 1, 16)] + (broadcast((float32*)A_2[(((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + k.outer)], 16)*(float32x16*)auto_scheduler_layout_transform[ramp(((floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*65536) + (k.outer*64)), 1, 16)]))
              matmul[ramp(((((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + (floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*64)) + 16), 1, 16)] = ((float32x16*)matmul[ramp(((((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + (floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*64)) + 16), 1, 16)] + (broadcast((float32*)A_2[(((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + k.outer)], 16)*(float32x16*)auto_scheduler_layout_transform[ramp((((floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*65536) + (k.outer*64)) + 16), 1, 16)]))
              matmul[ramp(((((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + (floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*64)) + 32), 1, 16)] = ((float32x16*)matmul[ramp(((((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + (floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*64)) + 32), 1, 16)] + (broadcast((float32*)A_2[(((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + k.outer)], 16)*(float32x16*)auto_scheduler_layout_transform[ramp((((floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*65536) + (k.outer*64)) + 32), 1, 16)]))
              matmul[ramp(((((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + (floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*64)) + 48), 1, 16)] = ((float32x16*)matmul[ramp(((((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + (floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*64)) + 48), 1, 16)] + (broadcast((float32*)A_2[(((floordiv(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*4096) + (i.outer.outer.inner*1024)) + k.outer)], 16)*(float32x16*)auto_scheduler_layout_transform[ramp((((floormod(i.outer.outer.outer.j.outer.outer.outer.fused, 16)*65536) + (k.outer*64)) + 48), 1, 16)]))
            }
          }
        }
        for (i: int32, 0, 1024) "parallel" {
          for (j: int32, 0, 1024) {
            out_2[((i*1024) + j)] = ((float32*)matmul[((i*1024) + j)] + (float32*)C_2[((i*1024) + j)])
          }
        }
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 127-130

Check correctness and evaluate performance
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We build the binary and check its correctness and performance.

.. GENERATED FROM PYTHON SOURCE LINES 130-155

.. code-block:: default


    func = tvm.build(sch, args, target)
    a_np = np.random.uniform(size=(N, L)).astype(np.float32)
    b_np = np.random.uniform(size=(L, M)).astype(np.float32)
    c_np = np.random.uniform(size=(N, M)).astype(np.float32)
    out_np = a_np.dot(b_np) + c_np

    ctx = tvm.cpu()
    a_tvm = tvm.nd.array(a_np, ctx=ctx)
    b_tvm = tvm.nd.array(b_np, ctx=ctx)
    c_tvm = tvm.nd.array(c_np, ctx=ctx)
    out_tvm = tvm.nd.empty(out_np.shape, ctx=ctx)
    func(a_tvm, b_tvm, c_tvm, out_tvm)

    # Check results
    np.testing.assert_allclose(out_np, out_tvm.asnumpy(), rtol=1e-3)

    # Evaluate execution time.
    evaluator = func.time_evaluator(func.entry_name, ctx, min_repeat_ms=500)
    print(
        "Execution time of this operator: %.3f ms"
        % (np.median(evaluator(a_tvm, b_tvm, c_tvm, out_tvm).results) * 1000)
    )






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Execution time of this operator: 25.649 ms




.. GENERATED FROM PYTHON SOURCE LINES 156-161

Using the record file
^^^^^^^^^^^^^^^^^^^^^
During the search, all measurement records are dumped into the record
file "matmul.json". The measurement records can be used to re-apply search results,
resume the search, and perform other analyses.

.. GENERATED FROM PYTHON SOURCE LINES 163-166

Here is an example where we load the best schedule from a file,
and print the equivalent python schedule API. This can be used for
debugging and learning the behavior of the auto-scheduler.

.. GENERATED FROM PYTHON SOURCE LINES 166-170

.. code-block:: default


    print("Equivalent python schedule:")
    print(task.print_best(log_file))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Equivalent python schedule:
    matmul_i, matmul_j, matmul_k = tuple(matmul.op.axis) + tuple(matmul.op.reduce_axis)
    out_i, out_j = tuple(out.op.axis) + tuple(out.op.reduce_axis)
    matmul_i_o_i, matmul_i_i = s[matmul].split(matmul_i, factor=1)
    matmul_i_o_o_i, matmul_i_o_i = s[matmul].split(matmul_i_o_i, factor=1)
    matmul_i_o_o_o, matmul_i_o_o_i = s[matmul].split(matmul_i_o_o_i, factor=4)
    matmul_j_o_i, matmul_j_i = s[matmul].split(matmul_j, factor=16)
    matmul_j_o_o_i, matmul_j_o_i = s[matmul].split(matmul_j_o_i, factor=4)
    matmul_j_o_o_o, matmul_j_o_o_i = s[matmul].split(matmul_j_o_o_i, factor=1)
    matmul_k_o, matmul_k_i = s[matmul].split(matmul_k, factor=1)
    s[matmul].reorder(matmul_i_o_o_o, matmul_j_o_o_o, matmul_i_o_o_i, matmul_j_o_o_i, matmul_k_o, matmul_i_o_i, matmul_j_o_i, matmul_k_i, matmul_i_i, matmul_j_i)
    matmul_i_o_o_o_j_o_o_o_fused = s[matmul].fuse(matmul_i_o_o_o, matmul_j_o_o_o)
    s[matmul].parallel(matmul_i_o_o_o_j_o_o_o_fused)
    s[out].parallel(out_i)
    s[matmul].pragma(matmul_i_o_o_o_j_o_o_o_fused, "auto_unroll_max_step", 64)
    s[matmul].pragma(matmul_i_o_o_o_j_o_o_o_fused, "unroll_explicit", True)
    s[matmul].vectorize(matmul_j_i)





.. GENERATED FROM PYTHON SOURCE LINES 171-175

A more complicated example is to resume the search.
In this case, we need to create the search policy and cost model by ourselves
and resume the status of search policy and cost model with the log file.
In the example below we resume the status and do more 5 trials.

.. GENERATED FROM PYTHON SOURCE LINES 175-191

.. code-block:: default



    def resume_search(task, log_file):
        print("Resume search:")
        cost_model = auto_scheduler.XGBModel()
        cost_model.update_from_file(log_file)
        search_policy = auto_scheduler.SketchPolicy(
            task, cost_model, init_search_callbacks=[auto_scheduler.PreloadMeasuredStates(log_file)]
        )
        tune_option = auto_scheduler.TuningOptions(
            num_measure_trials=5, measure_callbacks=[auto_scheduler.RecordToFile(log_file)]
        )
        task.tune(tune_option, search_policy=search_policy)


    resume_search(task, log_file)




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Resume search:
    /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/venv/lib/python3.7/site-packages/xgboost/training.py:19: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
      warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)






.. _sphx_glr_download_tutorials_auto_scheduler_tune_matmul_x86.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: tune_matmul_x86.py <tune_matmul_x86.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: tune_matmul_x86.ipynb <tune_matmul_x86.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
