
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/frontend/deploy_quantized.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorials_frontend_deploy_quantized.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_frontend_deploy_quantized.py:


Deploy a Quantized Model on Cuda
================================
**Author**: `Wuwei Lin <https://github.com/vinx13>`_

This article is an introductory tutorial of automatic quantization with TVM.
Automatic quantization is one of the quantization modes in TVM. More details on
the quantization story in TVM can be found
`here <https://discuss.tvm.apache.org/t/quantization-story/3920>`_.
In this tutorial, we will import a GluonCV pre-trained model on ImageNet to
Relay, quantize the Relay model and then perform the inference.

.. GENERATED FROM PYTHON SOURCE LINES 29-44

.. code-block:: default


    import tvm
    from tvm import te
    from tvm import relay
    import mxnet as mx
    from tvm.contrib.download import download_testdata
    from mxnet import gluon
    import logging
    import os

    batch_size = 1
    model_name = "resnet18_v1"
    target = "cuda"
    ctx = tvm.context(target)








.. GENERATED FROM PYTHON SOURCE LINES 45-49

Prepare the Dataset
-------------------
We will demonstrate how to prepare the calibration dataset for quantization.
We first download the validation set of ImageNet and pre-process the dataset.

.. GENERATED FROM PYTHON SOURCE LINES 49-80

.. code-block:: default

    calibration_rec = download_testdata(
        "http://data.mxnet.io.s3-website-us-west-1.amazonaws.com/data/val_256_q90.rec",
        "val_256_q90.rec",
    )


    def get_val_data(num_workers=4):
        mean_rgb = [123.68, 116.779, 103.939]
        std_rgb = [58.393, 57.12, 57.375]

        def batch_fn(batch):
            return batch.data[0].asnumpy(), batch.label[0].asnumpy()

        img_size = 299 if model_name == "inceptionv3" else 224
        val_data = mx.io.ImageRecordIter(
            path_imgrec=calibration_rec,
            preprocess_threads=num_workers,
            shuffle=False,
            batch_size=batch_size,
            resize=256,
            data_shape=(3, img_size, img_size),
            mean_r=mean_rgb[0],
            mean_g=mean_rgb[1],
            mean_b=mean_rgb[2],
            std_r=std_rgb[0],
            std_g=std_rgb[1],
            std_b=std_rgb[2],
        )
        return val_data, batch_fn






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    File /home/sunchul/.tvm_test_data/val_256_q90.rec exists, skip.




.. GENERATED FROM PYTHON SOURCE LINES 81-84

The calibration dataset should be an iterable object. We define the
calibration dataset as a generator object in Python. In this tutorial, we
only use a few samples for calibration.

.. GENERATED FROM PYTHON SOURCE LINES 84-98

.. code-block:: default


    calibration_samples = 10


    def calibrate_dataset():
        val_data, batch_fn = get_val_data()
        val_data.reset()
        for i, batch in enumerate(val_data):
            if i * batch_size >= calibration_samples:
                break
            data, _ = batch_fn(batch)
            yield {"data": data}









.. GENERATED FROM PYTHON SOURCE LINES 99-102

Import the model
----------------
We use the Relay MxNet frontend to import a model from the Gluon model zoo.

.. GENERATED FROM PYTHON SOURCE LINES 102-110

.. code-block:: default

    def get_model():
        gluon_model = gluon.model_zoo.vision.get_model(model_name, pretrained=True)
        img_size = 299 if model_name == "inceptionv3" else 224
        data_shape = (batch_size, 3, img_size, img_size)
        mod, params = relay.frontend.from_mxnet(gluon_model, {"data": data_shape})
        return mod, params









.. GENERATED FROM PYTHON SOURCE LINES 111-132

Quantize the Model
------------------
In quantization, we need to find the scale for each weight and intermediate
feature map tensor of each layer.

For weights, the scales are directly calculated based on the value of the
weights. Two modes are supported: `power2` and `max`. Both modes find the
maximum value within the weight tensor first. In `power2` mode, the maximum
is rounded down to power of two. If the scales of both weights and
intermediate feature maps are power of two, we can leverage bit shifting for
multiplications. This make it computationally more efficient. In `max` mode,
the maximum is used as the scale. Without rounding, `max` mode might have
better accuracy in some cases. When the scales are not powers of two, fixed
point multiplications will be used.

For intermediate feature maps, we can find the scales with data-aware
quantization. Data-aware quantization takes a calibration dataset as the
input argument. Scales are calculated by minimizing the KL divergence between
distribution of activation before and after quantization.
Alternatively, we can also use pre-defined global scales. This saves the time
for calibration. But the accuracy might be impacted.

.. GENERATED FROM PYTHON SOURCE LINES 132-144

.. code-block:: default



    def quantize(mod, params, data_aware):
        if data_aware:
            with relay.quantize.qconfig(calibrate_mode="kl_divergence", weight_scale="max"):
                mod = relay.quantize.quantize(mod, params, dataset=calibrate_dataset())
        else:
            with relay.quantize.qconfig(calibrate_mode="global_scale", global_scale=8.0):
                mod = relay.quantize.quantize(mod, params)
        return mod









.. GENERATED FROM PYTHON SOURCE LINES 145-148

Run Inference
-------------
We create a Relay VM to build and execute the model.

.. GENERATED FROM PYTHON SOURCE LINES 148-166

.. code-block:: default

    def run_inference(mod):
        executor = relay.create_executor("vm", mod, ctx, target)
        val_data, batch_fn = get_val_data()
        for i, batch in enumerate(val_data):
            data, label = batch_fn(batch)
            prediction = executor.evaluate()(data)
            if i > 10:  # only run inference on a few samples in this tutorial
                break


    def main():
        mod, params = get_model()
        mod = quantize(mod, params, data_aware=True)
        run_inference(mod)


    if __name__ == "__main__":
        main()


.. rst-class:: sphx-glr-script-out

.. code-block:: pytb

    Traceback (most recent call last):
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/tutorials/frontend/deploy_quantized.py", line 165, in <module>
        main()
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/tutorials/frontend/deploy_quantized.py", line 161, in main
        run_inference(mod)
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/tutorials/frontend/deploy_quantized.py", line 149, in run_inference
        executor = relay.create_executor("vm", mod, ctx, target)
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/relay/build_module.py", line 459, in create_executor
        return VMExecutor(mod, ctx, target)
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/relay/backend/vm.py", line 257, in __init__
        self.executable = compile(mod, target)
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/relay/backend/vm.py", line 69, in compile
        compiler.codegen()
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/relay/backend/vm.py", line 139, in codegen
        self._codegen()
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
        raise get_last_ffi_error()
    tvm._ffi.base.TVMError: Traceback (most recent call last):
      [bt] (6) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(TVMFuncCall+0x65) [0x7f14f98ff845]
      [bt] (5) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(+0x10bf8a9) [0x7f14f97af8a9]
      [bt] (4) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(tvm::relay::vm::VMCompiler::Codegen()+0x5e4) [0x7f14f97ae904]
      [bt] (3) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(tvm::build(tvm::Map<tvm::runtime::String, tvm::IRModule, void, void> const&, tvm::Target const&)+0xdf) [0x7f14f91f708f]
      [bt] (2) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(tvm::build(tvm::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)+0x584) [0x7f14f91f6764]
      [bt] (1) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(tvm::codegen::Build(tvm::IRModule, tvm::Target)+0xe62) [0x7f14f9292f92]
      [bt] (0) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(+0xba1e42) [0x7f14f9291e42]
      File "/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/src/target/codegen.cc", line 58
    TVMError: 
    ---------------------------------------------------------------
    An internal invariant was violated during the execution of TVM.
    Please read TVM's error reporting guidelines.
    More details can be found here: https://discuss.tvm.ai/t/error-reporting/7793.
    ---------------------------------------------------------------
      Check failed: bf != nullptr == false: target.build.cuda is not enabled





.. _sphx_glr_download_tutorials_frontend_deploy_quantized.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: deploy_quantized.py <deploy_quantized.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: deploy_quantized.ipynb <deploy_quantized.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
