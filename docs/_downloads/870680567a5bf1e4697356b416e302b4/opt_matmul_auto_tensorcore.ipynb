{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# How to optimize matmul with Auto TensorCore CodeGen\n**Author**: `Minmin Sun <https://github.com/minminsun>`_,             `Lanbo Li <https://github.com/Orion34C>`_,             `Chenfan Jia <https://github.com/jcf94>`_,             `Jun Yang <https://github.com/yangjunpro>`_\n\nIn this tutorial, we will demonstrate how to write a high performance matmul\nschedule on Volta/Turing GPUs with TVM Auto TensorCore CodeGen.\nThis is a transparent solution to generate tensorcore kernel\nwith most transformations done in ir passes.\nUsers can also write schedule with tensorization to generate TensorCore code.\nBoth solutions use the same tensorcore intrinsics.\nPlease refer to `opt-conv-tensorcore` tutorial for more details.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparation and Algorithm\n2 kinds of input data types are supported: float16 and int8.\nFor float16, the accumulator is float32.\nFor int8, the accumulator is int32.\nFor data layouts, 'N' means None-transpose while 'T' means Transpose.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport sys\n\nimport numpy as np\nimport tvm\nfrom tvm import te\n\nfrom tvm import autotvm\nfrom tvm.contrib import nvcc\n\n\ndef matmul_nn(A, B, L, dtype=\"float16\", layout=\"NN\"):\n    k = te.reduce_axis((0, L), name=\"k\")\n    if dtype == \"float16\":\n        out_type = \"float\"\n    elif dtype == \"int8\":\n        out_type = \"int\"\n    elif dtype == \"int4\" or dtype == \"int1\":\n        out_type = \"int\"\n    if layout == \"NN\":\n        return te.compute(\n            (N, M), lambda i, j: te.sum(A[i, k].astype(out_type) * B[k, j].astype(out_type), axis=k)\n        )\n    if layout == \"NT\":\n        return te.compute(\n            (N, M), lambda i, j: te.sum(A[k, i].astype(out_type) * B[k, j].astype(out_type), axis=k)\n        )\n    if layout == \"TN\":\n        return te.compute(\n            (N, M), lambda i, j: te.sum(A[i, k].astype(out_type) * B[j, k].astype(out_type), axis=k)\n        )\n    if layout == \"TT\":\n        return te.compute(\n            (N, M), lambda i, j: te.sum(A[k, i].astype(out_type) * B[j, k].astype(out_type), axis=k)\n        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scheduling the Computation\nThis schedule is no different than a non-tensorcore matmul schedule on GPU.\nPlease refer to `opt-gemm` tutorial for basics of optimizing matmul schedule.\nWhen the \"tensor_core\" pragma is set, the \"rewrite for tensorcore\" ir pass\nwill automatically transform the schedule for tensorcore codegen,\notherwise normal CUDA code, with lower performance but equal functionality, will be generated.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>*Requirements of TesnsorCore*\n\n  Note that in the following 2 cases, even though the \"tensor_core\" pragma is set, TVM will still fall back to normal CUDA codegen:\n  (1) The m, n or k of input matrices is not multiple of 16;\n  (2) The warp tile size is not 16x16x16 on CUDA9, or not one of {16x16x16, 32x8x16, 8x32x16} on CUDA version >= 10.0.</p></div>\n\nIn this schedule, storage_align is used to reduce bank conflicts of shared memory. Please refer to this\n`doc <https://tvm.apache.org/docs/api/python/te.html#tvm.te.Stage.storage_align>`_\nfor the usage of storage_align primitive. In short, we need to add an offset to some shared memory buffer\nto reduce bank conflicts.\nAccording to the `wmma doc <https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#wmma-description>`_,\nthe stride of load_matrix_sync must be a multiple of 16 bytes,\nso we choose 8 as offset for float16 and 16 as offset for int8.\n\nWe use AutoTVM to search for best configurations in this schedule.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@autotvm.template(\"tutorial/auto_tensorcore/test_gemm\")\ndef test_gemm(N, L, M, dtype, layout):\n    if layout == \"NN\":\n        shape_a = (N, L)\n        shape_b = (L, M)\n    elif layout == \"NT\":\n        shape_a = (L, N)\n        shape_b = (L, M)\n    elif layout == \"TN\":\n        shape_a = (N, L)\n        shape_b = (M, L)\n    elif layout == \"TT\":\n        shape_a = (L, N)\n        shape_b = (M, L)\n    else:\n        print(\"Unsupported layout:\", layout)\n        sys.exit(1)\n    A = te.placeholder(shape_a, name=\"A\", dtype=dtype)\n    B = te.placeholder(shape_b, name=\"B\", dtype=dtype)\n    C = matmul_nn(A, B, L, dtype, layout)\n\n    s = te.create_schedule(C.op)\n    y, x = s[C].op.axis\n    k = s[C].op.reduce_axis[0]\n\n    # storage_align params\n    factor = 16\n    offset = 8\n    if dtype == \"int8\":\n        factor = 32\n        offset = 16\n    elif dtype == \"int4\":\n        factor = 64\n        offset = 32\n    elif dtype == \"int1\":\n        factor = 256\n        offset = 128\n\n    # create cache stages\n    AA = s.cache_read(A, \"shared\", [C])\n    if layout == \"NN\" or layout == \"TN\":\n        s[AA].storage_align(AA.op.axis[0], factor, offset)\n    AL = s.cache_read(AA, \"local\", [C])\n    BB = s.cache_read(B, \"shared\", [C])\n    if layout == \"TT\" or layout == \"NT\":\n        s[BB].storage_align(BB.op.axis[0], factor, offset)\n    BL = s.cache_read(BB, \"local\", [C])\n    CL = s.cache_write(C, \"local\")\n\n    # autotvm search space definition\n    cfg = autotvm.get_config()\n\n    cfg.define_knob(\"bx\", [2, 4, 8])\n    cfg.define_knob(\"by\", [8, 16, 32, 64])\n    cfg.define_knob(\"step_k\", [1, 2, 4, 8, 16, 32])\n    cfg.define_knob(\"v\", [4, 8, 16, 32])\n    by = cfg[\"by\"].val\n    bx = cfg[\"bx\"].val\n    step_k = cfg[\"step_k\"].val\n    v = cfg[\"v\"].val\n\n    # thread tile\n    TX = 8\n    TY = 1\n    if dtype == \"int4\" or dtype == \"int1\":\n        TX = 2\n    # warp tile\n    warp_tile_m = 16  # it could also be 8 or 32 on CUDA version >= 10.0\n    warp_tile_k = 16  # it must be 16 for fp16/int8 data type\n    if dtype == \"int4\":\n        warp_tile_m = 8\n        warp_tile_k = 32\n    elif dtype == \"int1\":\n        warp_tile_m = 8\n        warp_tile_k = 128\n    # block tile\n    tile_x = bx * TX\n    tile_y = by * TY\n\n    yo, ty = s[C].split(y, tile_y)\n    ty, yi = s[C].split(ty, TY)\n\n    # schedule for C stage\n    xo, xi = s[C].split(x, tile_x)\n    WX = min(warp_tile_m, tile_x)\n    tz, xi = s[C].split(xi, WX)\n    tx, xi = s[C].split(xi, TX)\n    s[C].reorder(yo, xo, tz, ty, tx, yi, xi)\n    s[C].bind(yo, te.thread_axis(\"blockIdx.y\"))\n    s[C].bind(xo, te.thread_axis(\"blockIdx.x\"))\n    s[C].bind(ty, te.thread_axis(\"threadIdx.y\"))\n    s[C].bind(tz, te.thread_axis(\"threadIdx.z\"))\n    s[C].bind(tx, te.thread_axis(\"threadIdx.x\"))\n\n    # schedule for CL stage\n    ko, ki = s[CL].split(k, step_k * warp_tile_k)\n    kl, ki = s[CL].split(ki, warp_tile_k)\n    s[CL].compute_at(s[C], tx)\n    yo, xo = CL.op.axis\n    s[CL].reorder(ko, kl, ki, yo, xo)\n\n    # schedule for AA stage\n    s[AA].compute_at(s[CL], ko)\n    xo, xi = s[AA].split(s[AA].op.axis[1], factor=bx * v)\n    tz, tx = s[AA].split(xi, factor=(WX // TX) * v)\n    tx, vec = s[AA].split(tx, factor=v)\n    fused = s[AA].fuse(s[AA].op.axis[0], xo)\n    _, ty = s[AA].split(fused, factor=by)\n    s[AA].bind(ty, te.thread_axis(\"threadIdx.y\"))\n    s[AA].bind(tz, te.thread_axis(\"threadIdx.z\"))\n    s[AA].bind(tx, te.thread_axis(\"threadIdx.x\"))\n    # vectorization is very important for float16/int8 inputs\n    s[AA].vectorize(vec)\n\n    # schedule for BB stage\n    s[BB].compute_at(s[CL], ko)\n    xo, xi = s[BB].split(s[BB].op.axis[1], factor=bx * v)\n    tz, tx = s[BB].split(xi, factor=(WX // TX) * v)\n    tx, vec = s[BB].split(tx, factor=v)\n    fused = s[BB].fuse(s[BB].op.axis[0], xo)\n    _, ty = s[BB].split(fused, factor=by)\n    s[BB].bind(ty, te.thread_axis(\"threadIdx.y\"))\n    s[BB].bind(tz, te.thread_axis(\"threadIdx.z\"))\n    s[BB].bind(tx, te.thread_axis(\"threadIdx.x\"))\n    s[BB].vectorize(vec)\n\n    s[AL].compute_at(s[CL], kl)\n    s[BL].compute_at(s[CL], kl)\n\n    # set the 'tensor_core' pragma for tensorcore codegen\n    s[CL].pragma(ko, \"tensor_core\")\n\n    return s, [A, B, C]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AutoTune and Test\nFinally we use a tuner to tune the schedule, generate code with best config\nand run the kernel to compare with numpy to check whether the results are correct.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# check whether the gpu has tensorcore\nif not tvm.gpu(0).exist or not tvm.runtime.enabled(\"cuda\"):\n    raise Exception(\"skip building this tutorial because cuda is not enabled..\")\n\nctx = tvm.gpu()\nif not nvcc.have_tensorcore(ctx.compute_version):\n    raise Exception(\"the gpu has no tensorcore, skipping...\")\n\nM, N, L = 512, 32, 512\ndtype = \"float16\"\nlayout = \"NN\"\nif len(sys.argv) >= 4:\n    M, N, L = int(sys.argv[1]), int(sys.argv[2]), int(sys.argv[3])\nif len(sys.argv) >= 5:\n    dtype = sys.argv[4]\nif len(sys.argv) >= 6:\n    layout = sys.argv[5]\n\n# check whether current gpu arch support support current dtype's wmma codegen\ncuda_compute_capability = tvm.runtime._ffi_api.GetDeviceAttr(2, 0, 4)\nmajor, minor = nvcc.parse_compute_version(cuda_compute_capability)\nif dtype == \"int8\":\n    assert major == 7 and minor >= 2\nelif dtype == \"int4\" or dtype == \"int1\":\n    # int4/int1 only support layout TN\n    assert major == 7 and minor == 5 and layout == \"TN\"\n\n\ndef tune_and_evaluate(M, N, L, dtype, layout):\n    task = autotvm.task.create(\n        \"tutorial/auto_tensorcore/test_gemm\", args=(N, L, M, dtype, layout), target=\"cuda\"\n    )\n    print(task.config_space)\n\n    logging.getLogger(\"autotvm\").setLevel(logging.DEBUG)\n    logging.getLogger(\"autotvm\").addHandler(logging.StreamHandler(sys.stdout))\n\n    measure_option = autotvm.measure_option(builder=\"local\", runner=autotvm.LocalRunner(number=5))\n\n    tuner = autotvm.tuner.XGBTuner(task)\n    tuner.tune(\n        n_trial=1000,\n        measure_option=measure_option,\n        callbacks=[autotvm.callback.log_to_file(\"matmul.log\")],\n    )\n\n    dispatch_context = autotvm.apply_history_best(\"matmul.log\")\n    best_config = dispatch_context.query(task.target, task.workload)\n    print(\"\\nBest config:\")\n    print(best_config)\n    with autotvm.apply_history_best(\"matmul.log\"):\n        with tvm.target.Target(\"cuda\"):\n            s, arg_bufs = test_gemm(N, L, M, dtype, layout)\n            print(tvm.lower(s, arg_bufs, simple_mode=True))\n            func = tvm.build(s, arg_bufs)\n    dev_module = func.imported_modules[0]\n    print(dev_module.get_source())\n\n    # check correctness\n    if layout == \"NN\":\n        shape_a = (N, L)\n        shape_b = (L, M)\n    elif layout == \"NT\":\n        shape_a = (L, N)\n        shape_b = (L, M)\n    elif layout == \"TN\":\n        shape_a = (N, L)\n        shape_b = (M, L)\n    elif layout == \"TT\":\n        shape_a = (L, N)\n        shape_b = (M, L)\n\n    a_np = None\n    b_np = None\n    c_np = None\n    c_np_type = None\n    if dtype == \"float16\":\n        c_np_type = np.float32\n        a_np = np.random.uniform(size=shape_a).astype(np.float16)\n        b_np = np.random.uniform(size=shape_b).astype(np.float16)\n        if layout == \"NN\":\n            c_np = np.dot(a_np, b_np)\n        elif layout == \"NT\":\n            c_np = np.dot(a_np.T, b_np)\n        elif layout == \"TN\":\n            c_np = np.dot(a_np, b_np.T)\n        elif layout == \"TT\":\n            c_np = np.dot(a_np.T, b_np.T)\n    elif dtype == \"int8\":\n        c_np_type = np.int32\n        a_np = np.random.randint(low=-128, high=127, size=shape_a).astype(np.int8)\n        b_np = np.random.randint(low=-128, high=127, size=shape_b).astype(np.int8)\n        if layout == \"NN\":\n            c_np = np.dot(a_np.astype(np.int32), b_np.astype(np.int32))\n        elif layout == \"NT\":\n            c_np = np.dot(a_np.astype(np.int32).T, b_np.astype(np.int32))\n        elif layout == \"TN\":\n            c_np = np.dot(a_np.astype(np.int32), b_np.astype(np.int32).T)\n        elif layout == \"TT\":\n            c_np = np.dot(a_np.astype(np.int32).T, b_np.astype(np.int32).T)\n    elif dtype == \"int4\":\n        c_np_type = np.int32\n        a_np_int = np.random.randint(low=-8, high=7, size=shape_a).astype(np.int32)\n        b_np_int = np.random.randint(low=-8, high=7, size=shape_b).astype(np.int32)\n        # \"TN\"\n        c_np = np.dot(a_np_int.astype(np.int32), b_np_int.astype(np.int32).T)\n        a_np = np.zeros(shape=(N, int(L / 8)), dtype=np.int32)\n        b_np = np.zeros(shape=(M, int(L / 8)), dtype=np.int32)\n        # a_np --> col_major\n        for i in range(N):\n            for j in range(int(L / 8)):\n                for k in range(8):\n                    a_np[i, j] = a_np[i, j] | ((a_np_int[i, j * 8 + k] & 0xF) << ((7 - k) * 4))\n\n        # b_np --> row_major\n        for i in range(M):\n            for j in range(int(L / 8)):\n                for k in range(8):\n                    b_np[i, j] = b_np[i, j] | ((b_np_int[i, j * 8 + k] & 0xF) << ((7 - k) * 4))\n    elif dtype == \"int1\":\n        c_np_type = np.int32\n        a_np_int = np.random.randint(low=0, high=1, size=shape_a).astype(np.int32)\n        b_np_int = np.random.randint(low=0, high=1, size=shape_b).astype(np.int32)\n        # \"TN\"\n        c_np = np.dot(a_np_int.astype(np.int32), b_np_int.astype(np.int32).T)\n        a_np = np.zeros(shape=(N, int(L / 32)), dtype=np.int32)\n        b_np = np.zeros(shape=(M, int(L / 32)), dtype=np.int32)\n        for i in range(N):\n            for j in range(int(L / 32)):\n                for k in range(32):\n                    a_np[i, j] = a_np[i, j] | ((a_np_int[i, j * 32 + k] & 0xF) << (31 - k))\n\n        for i in range(M):\n            for j in range(int(L / 32)):\n                for k in range(32):\n                    b_np[i, j] = b_np[i, j] | ((b_np_int[i, j * 32 + k] & 0xF) << (31 - k))\n\n    c_tvm = tvm.nd.array(np.zeros(c_np.shape, dtype=c_np_type), ctx=ctx)\n    a_tvm = tvm.nd.array(a_np, ctx=ctx)\n    b_tvm = tvm.nd.array(b_np, ctx=ctx)\n    func(a_tvm, b_tvm, c_tvm)\n\n    tvm.testing.assert_allclose(c_np, c_tvm.asnumpy(), rtol=1e-3)\n\n    evaluator = func.time_evaluator(func.entry_name, ctx, number=100)\n    print(\"Time cost of this operator: %f\" % evaluator(a_tvm, b_tvm, c_tvm).mean)\n\n\n# We do not run the tuning in our webpage server since it takes some time.\n# Uncomment the following line to run it by yourself.\n\n# tune_and_evaluate(M, N, L, dtype, layout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Output\n.. code-block:: bash\n\n   Best config:\n   [('bx', 4), ('by', 32), ('step_k', 16), ('v', 8)],,None,40\n   Finish loading 162 records\n   produce compute {\n     // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 1\n     // attr [compute.local] storage_scope = \"wmma.accumulator\"\n     allocate compute.local[float32 * 256]\n     // attr [A.shared] storage_scope = \"shared\"\n     allocate A.shared[float16 * 8448]\n     // attr [B.shared] storage_scope = \"shared\"\n     allocate B.shared[float16 * 8192]\n     // attr [A.shared.local] storage_scope = \"wmma.matrix_b\"\n     allocate A.shared.local[float16 * 256]\n     // attr [B.shared.local] storage_scope = \"wmma.matrix_a\"\n     allocate B.shared.local[float16 * 256]\n     // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 16\n     // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 2\n     // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 32\n     // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 2\n     produce compute.local {\n       for (j.c.init, 0, 1) {\n         tvm_fill_fragment(compute.local, 16, 16, 16, 0, 0f)\n       }\n       // attr [iter_var(k.outer, )] pragma_tensor_core = 1\n       for (k.outer, 0, 2) {\n         produce A.shared {\n           for (ax0.ax1.outer.fused.outer, 0, 8) {\n             // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 32\n             // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 2\n             // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 2\n             A.shared[ramp((((((ax0.ax1.outer.fused.outer*1056) + (floordiv(threadIdx.y, 8)*264)) + (floormod(threadIdx.y, 8)*32)) + (threadIdx.z*16)) + (threadIdx.x*8)), 1, 8)] = A[ramp(((((((ax0.ax1.outer.fused.outer*2048) + (floordiv(threadIdx.y, 8)*512)) + (k.outer*256)) + (floormod(threadIdx.y, 8)*32)) + (threadIdx.z*16)) + (threadIdx.x*8)), 1, 8)]\n           }\n         }\n         produce B.shared {\n           for (ax0.ax1.outer.fused.outer, 0, 8) {\n             // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 32\n             // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 2\n             // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 2\n             B.shared[ramp(((((ax0.ax1.outer.fused.outer*1024) + (threadIdx.y*32)) + (threadIdx.z*16)) + (threadIdx.x*8)), 1, 8)] = B[ramp(((((((k.outer*131072) + (ax0.ax1.outer.fused.outer*16384)) + (threadIdx.y*512)) + (blockIdx.x*32)) + (threadIdx.z*16)) + (threadIdx.x*8)), 1, 8)]\n           }\n         }\n         for (k.inner.outer, 0, 16) {\n           produce A.shared.local {\n             for (ax1, 0, 1) {\n               tvm_load_matrix_sync(A.shared.local, 16, 16, 16, 0, &(A.shared[(((threadIdx.y/16)*4224) + (k.inner.outer*16))]), 264, \"col_major\")\n             }\n           }\n           produce B.shared.local {\n             for (ax0, 0, 1) {\n               for (ax1, 0, 1) {\n                 tvm_load_matrix_sync(B.shared.local, 16, 16, 16, 0, &(B.shared[((k.inner.outer*512) + (threadIdx.z*16))]), 32, \"col_major\")\n               }\n             }\n           }\n           for (k.inner.inner, 0, 1) {\n             for (j.c, 0, 1) {\n               tvm_mma_sync(compute.local, 0, B.shared.local, 0, A.shared.local, 0, compute.local, 0)\n             }\n           }\n         }\n       }\n     }\n     for (j.inner.inner.inner, 0, 1) {\n       tvm_store_matrix_sync(compute.local, 16, 16, 16, 0, &(compute[((((threadIdx.y/16)*8192) + (blockIdx.x*32)) + (threadIdx.z*16))]), 512, \"col_major\")\n     }\n   }\n\n   #include <cuda_fp16.h>\n   __device__ half max(const half a, const half b)\n   {\n     return __hgt(__half(a), __half(b)) ? a : b;\n   }\n   __device__ half min(const half a, const half b)\n   {\n     return __hlt(__half(a), __half(b)) ? a : b;\n   }\n   __device__ half operator+(const volatile __half &a,  const volatile __half &b)\n   {\n     return __hadd(a, b);\n   }\n   __device__ half operator<=(const volatile __half &a,  const volatile __half &b)\n   {\n     return __hlt(a, b);\n   }\n   __device__ half operator*(const volatile __half &a,  const volatile __half &b)\n   {\n     return __hmul(a, b);\n   }\n   #include <mma.h>\n   extern \"C\" __global__ void default_function_kernel0( half* __restrict__ A,  half* __restrict__ B,  float* __restrict__ compute) {\n     nvcuda::wmma::fragment<nvcuda::wmma::accumulator, 16, 16, 16, float> compute_local[1];\n     __shared__ half A_shared[8448];\n     __shared__ half B_shared[8192];\n     nvcuda::wmma::fragment<nvcuda::wmma::matrix_b, 16, 16, 16, half, nvcuda::wmma::col_major> A_shared_local[1];\n     nvcuda::wmma::fragment<nvcuda::wmma::matrix_a, 16, 16, 16, half, nvcuda::wmma::col_major> B_shared_local[1];\n     for (int j_c_init = 0; j_c_init < 1; ++j_c_init) {\n       (void)nvcuda::wmma::fill_fragment(compute_local[0], 0.000000e+00f);\n     }\n     for (int k_outer = 0; k_outer < 2; ++k_outer) {\n       __syncthreads();\n       for (int ax0_ax1_outer_fused_outer = 0; ax0_ax1_outer_fused_outer < 8; ++ax0_ax1_outer_fused_outer) {\n         ((__shared__ float4*)(A_shared + (((((ax0_ax1_outer_fused_outer * 1056) + ((((int)threadIdx.y) >> 3) * 264)) + ((((int)threadIdx.y) & 7) * 32)) + (((int)threadIdx.z) * 16)) + (((int)threadIdx.x) * 8))))[0] = (( float4*)(A + ((((((ax0_ax1_outer_fused_outer * 2048) + ((((int)threadIdx.y) >> 3) * 512)) + (k_outer * 256)) + ((((int)threadIdx.y) & 7) * 32)) + (((int)threadIdx.z) * 16)) + (((int)threadIdx.x) * 8))))[0];\n       }\n       for (int ax0_ax1_outer_fused_outer1 = 0; ax0_ax1_outer_fused_outer1 < 8; ++ax0_ax1_outer_fused_outer1) {\n         ((__shared__ float4*)(B_shared + ((((ax0_ax1_outer_fused_outer1 * 1024) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 16)) + (((int)threadIdx.x) * 8))))[0] = (( float4*)(B + ((((((k_outer * 131072) + (ax0_ax1_outer_fused_outer1 * 16384)) + (((int)threadIdx.y) * 512)) + (((int)blockIdx.x) * 32)) + (((int)threadIdx.z) * 16)) + (((int)threadIdx.x) * 8))))[0];\n       }\n       __syncthreads();\n       for (int k_inner_outer = 0; k_inner_outer < 16; ++k_inner_outer) {\n         for (int ax1 = 0; ax1 < 1; ++ax1) {\n           (void)nvcuda::wmma::load_matrix_sync(A_shared_local[0], &(A_shared[(((((int)threadIdx.y) / 16) * 4224) + (k_inner_outer * 16))]), 264);\n         }\n         for (int ax0 = 0; ax0 < 1; ++ax0) {\n           for (int ax11 = 0; ax11 < 1; ++ax11) {\n             (void)nvcuda::wmma::load_matrix_sync(B_shared_local[0], &(B_shared[((k_inner_outer * 512) + (((int)threadIdx.z) * 16))]), 32);\n           }\n         }\n         for (int k_inner_inner = 0; k_inner_inner < 1; ++k_inner_inner) {\n           for (int j_c = 0; j_c < 1; ++j_c) {\n             (void)nvcuda::wmma::mma_sync(compute_local[0], B_shared_local[0], A_shared_local[0], compute_local[0]);\n           }\n         }\n       }\n     }\n     for (int j_inner_inner_inner = 0; j_inner_inner_inner < 1; ++j_inner_inner_inner) {\n       (void)nvcuda::wmma::store_matrix_sync(&(compute[((((((int)threadIdx.y) / 16) * 8192) + (((int)blockIdx.x) * 32)) + (((int)threadIdx.z) * 16))]), compute_local[0], 512, nvcuda::wmma::mem_col_major);\n     }\n   }\n\n\n   Time cost of this operator: 0.000008\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\nThis tutorial demonstrates how to use the AutoTensorCoreCodeGen of TVM\nto generate tensorcore kernels.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}