{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Micro TVM with TFLite Models\n**Author**: `Tom Gall <https://github.com/tom-gall>`_\n\nThis tutorial is an introduction to working with MicroTVM and a TFLite\nmodel with Relay.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nTo get started, TFLite package needs to be installed as prerequisite.\n\ninstall tflite\n\n.. code-block:: bash\n\n  pip install tflite=2.1.0 --user\n\nor you could generate TFLite package yourself. The steps are the following:\n\n  Get the flatc compiler.\n  Please refer to https://github.com/google/flatbuffers for details\n  and make sure it is properly installed.\n\n.. code-block:: bash\n\n  flatc --version\n\nGet the TFLite schema.\n\n.. code-block:: bash\n\n  wget https://raw.githubusercontent.com/tensorflow/tensorflow/r1.13/tensorflow/lite/schema/schema.fbs\n\nGenerate TFLite package.\n\n.. code-block:: bash\n\n  flatc --python schema.fbs\n\nAdd the current folder (which contains generated tflite module) to PYTHONPATH.\n\n.. code-block:: bash\n\n  export PYTHONPATH=${PYTHONPATH:+$PYTHONPATH:}$(pwd)\n\nTo validate that the TFLite package was installed successfully, ``python -c \"import tflite\"``\n\nCMSIS needs to be downloaded and the CMSIS_ST_PATH environment variable setup\nThis tutorial only supports the STM32F7xx series of boards.\nDownload from : https://www.st.com/en/embedded-software/stm32cubef7.html\nAfter you've expanded the zip file\n\n.. code-block:: bash\n\n  export CMSIS_ST_PATH=/path/to/STM32Cube_FW_F7_V1.16.0/Drivers/CMSIS\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recreating your own Pre-Trained TFLite model\n\nThe tutorial downloads a pretrained TFLite model. When working with microcontrollers\nyou need to be mindful these are highly resource constrained devices as such standard\nmodels like MobileNet may not fit into their modest memory.\n\nFor this tutorial, we'll make use of one of the TF Micro example models.\n\nIf you wish to replicate the training steps see:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world/train\n\n  .. note::\n\n    If you accidentally download the example pretrained model from:\n    wget https://storage.googleapis.com/download.tensorflow.org/models/tflite/micro/hello_world_2020_04_13.zip\n    this will fail due to an unimplemented opcode (114)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport numpy as np\nimport tvm\nimport tvm.micro as micro\nfrom tvm.contrib.download import download_testdata\nfrom tvm.contrib import graph_runtime, utils\nfrom tvm import relay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and prepare the Pre-Trained Model\n\nLoad the pretrained TFLite model from a file in your current\ndirectory into a buffer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_url = \"https://people.linaro.org/~tom.gall/sine_model.tflite\"\nmodel_file = \"sine_model.tflite\"\nmodel_path = download_testdata(model_url, model_file, module=\"data\")\n\ntflite_model_buf = open(model_path, \"rb\").read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the buffer, transform into a tflite model python object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    import tflite\n\n    tflite_model = tflite.Model.GetRootAsModel(tflite_model_buf, 0)\nexcept AttributeError:\n    import tflite.Model\n\n    tflite_model = tflite.Model.Model.GetRootAsModel(tflite_model_buf, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print out the version of the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "version = tflite_model.Version()\nprint(\"Model Version: \" + str(version))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parse the python model object to convert it into a relay module\nand weights.\nIt is important to note that the input tensor name must match what\nis contained in the model.\n\nIf you are unsure what that might be, this can be discovered by using\nthe visualize.py script within the Tensorflow project.\nSee : How do I inspect a .tflite file? `<https://www.tensorflow.org/lite/guide/faq>`_\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_tensor = \"dense_4_input\"\ninput_shape = (1,)\ninput_dtype = \"float32\"\n\nmod, params = relay.frontend.from_tflite(\n    tflite_model, shape_dict={input_tensor: input_shape}, dtype_dict={input_tensor: input_dtype}\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create a build config for relay. turning off two options\nand then calling relay.build which will result in a C source\nfile.\n\n.. code-block:: python\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "TARGET = tvm.target.target.micro(\"host\")\n\nwith tvm.transform.PassContext(\n    opt_level=3, config={\"tir.disable_vectorize\": True}, disabled_pass=[\"FuseOps\"]\n):\n    graph, c_mod, c_params = relay.build(mod, target=TARGET, params=params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running on simulated device\n\nFirst, compile a static microTVM runtime for the targeted device. In this case, the host simulated\ndevice is used.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "workspace = tvm.micro.Workspace()\n\ncompiler = tvm.micro.DefaultCompiler(target=TARGET)\nopts = tvm.micro.default_options(os.path.join(tvm.micro.CRT_ROOT_DIR, \"host\"))\n\nmicro_binary = tvm.micro.build_static_runtime(\n    # the x86 compiler *expects* you to give the exact same dictionary for both\n    # lib_opts and bin_opts. so the library compiler is mutating lib_opts and\n    # the binary compiler is expecting those mutations to be in bin_opts.\n    # TODO(weberlo) fix this very bizarre behavior\n    workspace,\n    compiler,\n    c_mod,\n    lib_opts=opts[\"bin_opts\"],\n    bin_opts=opts[\"bin_opts\"],\n    # Use the microTVM memory manager. If, in your main.cc, you change TVMPlatformMemoryAllocate and\n    # TVMPlatformMemoryFree to use e.g. malloc() and free(), you can omit this extra library.\n    extra_libs=[os.path.join(tvm.micro.build.CRT_ROOT_DIR, \"memory\")],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, establish a session with the simulated device and run the\ncomputation. The `with session` line would typically flash an attached\nmicrocontroller, but in this tutorial, it simply launches a subprocess\nto stand in for an attached microcontroller.\n\n.. code-block:: python\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "flasher = compiler.flasher()\nwith tvm.micro.Session(binary=micro_binary, flasher=flasher) as session:\n    graph_mod = tvm.micro.create_local_graph_runtime(\n        graph, session.get_system_lib(), session.context\n    )\n\n    # Set the model parameters using the lowered parameters produced by `relay.build`.\n    graph_mod.set_input(**c_params)\n\n    # The model consumes a single float32 value and returns a predicted sine value.  To pass the\n    # input value we construct a tvm.nd.array object with a single contrived number as input. For\n    # this model values of 0 to 2Pi are acceptable.\n    graph_mod.set_input(input_tensor, tvm.nd.array(np.array([0.5], dtype=\"float32\")))\n    graph_mod.run()\n\n    tvm_output = graph_mod.get_output(0).asnumpy()\n    print(\"result is: \" + str(tvm_output))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}