





<!DOCTYPE html>
<html class="writer-html5" lang="kr" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Get Started with Tensor Expression &mdash; tvm 0.8.dev0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/tlcpack_theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-rendered-html.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/tvm-logo-square.png"/>
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Getting Started with TVM command line driver - TVMC" href="tvmc_command_line_driver.html" />
    <link rel="prev" title="Quick Start Tutorial for Compiling Deep Learning Models" href="relay_quick_start.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
               <a href="https://tvm.apache.org/"><img src=https://tvm.apache.org/assets/images/logo.svg alt="logo"></a>
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/community>Community</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/download>Download</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/vta>VTA</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/blog>Blog</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/docs>Docs</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvmconf.org>Conference</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/apache/tvm/>Github</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   ASF
                 </button>
                 <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/security/>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  ASF
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/security/>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/tvm-logo-small.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.8.dev0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">입문</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">설치</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contribute/index.html">TVM에 기여하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploy/index.html">구현과 탑재</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/how_to.html">Developer How-To Guide</a></li>
</ul>
<p class="caption"><span class="caption-text">튜토리얼</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Get Started Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="relay_quick_start.html">Quick Start Tutorial for Compiling Deep Learning Models</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Get Started with Tensor Expression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vector-add-example">Vector Add Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#describe-the-computation">Describe the Computation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#schedule-the-computation">Schedule the Computation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compilation">Compilation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#run-the-function">Run the Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inspect-the-generated-code">Inspect the Generated Code</a></li>
<li class="toctree-l3"><a class="reference internal" href="#save-compiled-module">Save Compiled Module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#load-compiled-module">Load Compiled Module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pack-everything-into-one-library">Pack Everything into One Library</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generate-opencl-code">Generate OpenCL Code</a></li>
<li class="toctree-l3"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="tvmc_command_line_driver.html">Getting Started with TVM command line driver - TVMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="cross_compilation_and_rpc.html">Cross Compilation and RPC</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#compile-deep-learning-models">Compile Deep Learning Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#tensor-expression-and-schedules">Tensor Expression and Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#optimize-tensor-operators">Optimize Tensor Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#autotvm-template-based-auto-tuning">AutoTVM : Template-based Auto Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#autoscheduler-template-free-auto-scheduling">AutoScheduler : Template-free Auto Scheduling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#developer-tutorials">Developer Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#topi-tvm-operator-inventory">TOPI: TVM Operator Inventory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#micro-tvm">Micro TVM</a></li>
</ul>
<p class="caption"><span class="caption-text">참고 자료</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../langref/index.html">언어 레퍼런스</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/index.html">파이썬 API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/links.html">다른 API 참조를 위한 링크</a></li>
</ul>
<p class="caption"><span class="caption-text">심층 해설</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dev/index.html">디자인과 아키텍쳐</a></li>
</ul>
<p class="caption"><span class="caption-text">기타</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../vta/index.html">VTA: 딥러닝 가속기 스택</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">자주 묻는 질문(FAQ)</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">인덱스</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- tvm -->
              Table of content
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> <span class="br-arrow">></span></li>
        
          <li><a href="../index.html">Get Started Tutorials</a> <span class="br-arrow">></span></li>
        
      <li>Get Started with Tensor Expression</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/tutorials/get_started/tensor_expr_get_started.rst.txt" rel="nofollow"> <img src="../../_static//img/source.svg" alt="viewsource"/></a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-tutorials-get-started-tensor-expr-get-started-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="get-started-with-tensor-expression">
<span id="tutorial-tensor-expr-get-started"></span><span id="sphx-glr-tutorials-get-started-tensor-expr-get-started-py"></span><h1>Get Started with Tensor Expression<a class="headerlink" href="#get-started-with-tensor-expression" title="Permalink to this headline">¶</a></h1>
<p><strong>Author</strong>: <a class="reference external" href="https://tqchen.github.io">Tianqi Chen</a></p>
<p>This is an introductory tutorial to the Tensor expression language in TVM.
TVM uses a domain specific tensor expression for efficient kernel construction.</p>
<p>In this tutorial, we will demonstrate the basic workflow to use
the tensor expression language.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span><span class="p">,</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">import</span> <span class="nn">tvm.testing</span>
<span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">te</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Global declarations of environment.</span>

<span class="n">tgt_host</span> <span class="o">=</span> <span class="s2">&quot;llvm&quot;</span>
<span class="c1"># Change it to respective GPU if gpu is enabled Ex: cuda, opencl, rocm</span>
<span class="n">tgt</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span>
</pre></div>
</div>
<div class="section" id="vector-add-example">
<h2>Vector Add Example<a class="headerlink" href="#vector-add-example" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial, we will use a vector addition example to demonstrate
the workflow.</p>
</div>
<div class="section" id="describe-the-computation">
<h2>Describe the Computation<a class="headerlink" href="#describe-the-computation" title="Permalink to this headline">¶</a></h2>
<p>As a first step, we need to describe our computation.
TVM adopts tensor semantics, with each intermediate result
represented as a multi-dimensional array. The user needs to describe
the computation rule that generates the tensors.</p>
<p>We first define a symbolic variable n to represent the shape.
We then define two placeholder Tensors, A and B, with given shape (n,)</p>
<p>We then describe the result tensor C, with a compute operation.  The
compute function takes the shape of the tensor, as well as a lambda
function that describes the computation rule for each position of
the tensor.</p>
<p>No computation happens during this phase, as we are only declaring how
the computation should be done.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;n&quot;</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">n</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;A&quot;</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">n</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;B&quot;</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">C</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;tvm.te.tensor.Tensor&#39;&gt;
</pre></div>
</div>
</div>
<div class="section" id="schedule-the-computation">
<h2>Schedule the Computation<a class="headerlink" href="#schedule-the-computation" title="Permalink to this headline">¶</a></h2>
<p>While the above lines describe the computation rule, we can compute
C in many ways since the axis of C can be computed in a data
parallel manner.  TVM asks the user to provide a description of the
computation called a schedule.</p>
<p>A schedule is a set of transformation of computation that transforms
the loop of computations in the program.</p>
<p>After we construct the schedule, by default the schedule computes
C in a serial manner in a row-major order.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
</pre></div>
</div>
<p>We used the split construct to split the first axis of C,
this will split the original iteration axis into product of
two iterations. This is equivalent to the following code.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">bx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">bx</span> <span class="o">&lt;</span> <span class="n">ceil</span><span class="p">(</span><span class="n">n</span> <span class="o">/</span> <span class="mi">64</span><span class="p">);</span> <span class="o">++</span><span class="n">bx</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">tx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">tx</span> <span class="o">&lt;</span> <span class="mi">64</span><span class="p">;</span> <span class="o">++</span><span class="n">tx</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">bx</span> <span class="o">*</span> <span class="mi">64</span> <span class="o">+</span> <span class="n">tx</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bx</span><span class="p">,</span> <span class="n">tx</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">C</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">factor</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally we bind the iteration axis bx and tx to threads in the GPU
compute grid. These are GPU specific constructs that allow us
to generate code that runs on GPU.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">tgt</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">or</span> <span class="n">tgt</span> <span class="o">==</span> <span class="s2">&quot;rocm&quot;</span> <span class="ow">or</span> <span class="n">tgt</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;opencl&quot;</span><span class="p">):</span>
    <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">]</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">bx</span><span class="p">,</span> <span class="n">te</span><span class="o">.</span><span class="n">thread_axis</span><span class="p">(</span><span class="s2">&quot;blockIdx.x&quot;</span><span class="p">))</span>
    <span class="n">s</span><span class="p">[</span><span class="n">C</span><span class="p">]</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">tx</span><span class="p">,</span> <span class="n">te</span><span class="o">.</span><span class="n">thread_axis</span><span class="p">(</span><span class="s2">&quot;threadIdx.x&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="compilation">
<h2>Compilation<a class="headerlink" href="#compilation" title="Permalink to this headline">¶</a></h2>
<p>After we have finished specifying the schedule, we can compile it
into a TVM function. By default TVM compiles into a type-erased
function that can be directly called from the python side.</p>
<p>In the following line, we use tvm.build to create a function.
The build function takes the schedule, the desired signature of the
function (including the inputs and outputs) as well as target language
we want to compile to.</p>
<p>The result of compilation fadd is a GPU device function (if GPU is
involved) as well as a host wrapper that calls into the GPU
function.  fadd is the generated host wrapper function, it contains
a reference to the generated device function internally.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fadd</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;myadd&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-pytb notranslate"><div class="highlight"><pre><span></span><span class="gt">Traceback (most recent call last):</span>
  File <span class="nb">&quot;/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/tutorials/get_started/tensor_expr_get_started.py&quot;</span>, line <span class="m">141</span>, in <span class="n">&lt;module&gt;</span>
    <span class="n">fadd</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">tgt_host</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;myadd&quot;</span><span class="p">)</span>
  File <span class="nb">&quot;/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/driver/build_module.py&quot;</span>, line <span class="m">416</span>, in <span class="n">build</span>
    <span class="n">mod_host</span><span class="p">,</span> <span class="n">mdev</span> <span class="o">=</span> <span class="n">_build_for_device</span><span class="p">(</span><span class="n">input_mod</span><span class="p">,</span> <span class="n">tar</span><span class="p">,</span> <span class="n">target_host</span><span class="p">)</span>
  File <span class="nb">&quot;/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/driver/build_module.py&quot;</span>, line <span class="m">297</span>, in <span class="n">_build_for_device</span>
    <span class="n">rt_mod_dev</span> <span class="o">=</span> <span class="n">codegen</span><span class="o">.</span><span class="n">build_module</span><span class="p">(</span><span class="n">mod_dev</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mod_dev</span><span class="o">.</span><span class="n">functions</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
  File <span class="nb">&quot;/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/target/codegen.py&quot;</span>, line <span class="m">39</span>, in <span class="n">build_module</span>
    <span class="k">return</span> <span class="n">_ffi_api</span><span class="o">.</span><span class="n">Build</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
  File <span class="nb">&quot;/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/docs/../python/tvm/_ffi/_ctypes/packed_func.py&quot;</span>, line <span class="m">237</span>, in <span class="n">__call__</span>
    <span class="k">raise</span> <span class="n">get_last_ffi_error</span><span class="p">()</span>
<span class="gr">tvm._ffi.base.TVMError</span>: <span class="n">Traceback (most recent call last):</span>
<span class="x">  [bt] (3) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(TVMFuncCall+0x65) [0x7f14f98ff845]</span>
<span class="x">  [bt] (2) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(std::_Function_handler&lt;void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc&lt;tvm::runtime::Module (tvm::IRModule, tvm::Target)&gt;::AssignTypedLambda&lt;tvm::runtime::Module (*)(tvm::IRModule, tvm::Target)&gt;(tvm::runtime::Module (*)(tvm::IRModule, tvm::Target))::{lambda(tvm::runtime::TVMArgs const&amp;, tvm::runtime::TVMRetValue*)#1}&gt;::_M_invoke(std::_Any_data const&amp;, tvm::runtime::TVMArgs&amp;&amp;, tvm::runtime::TVMRetValue*&amp;&amp;)+0x677) [0x7f14f9299617]</span>
<span class="x">  [bt] (1) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(tvm::codegen::Build(tvm::IRModule, tvm::Target)+0xe62) [0x7f14f9292f92]</span>
<span class="x">  [bt] (0) /home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/build/libtvm.so(+0xba1e42) [0x7f14f9291e42]</span>
  File <span class="nb">&quot;/home/sunchul/workspace/gitproj/tvmdoc.multilingual.io/desk/src/target/codegen.cc&quot;</span>, line <span class="m">58</span>
<span class="gr">TVMError</span>:
<span class="x">---------------------------------------------------------------</span>
<span class="x">An internal invariant was violated during the execution of TVM.</span>
<span class="x">Please read TVM&#39;s error reporting guidelines.</span>
<span class="x">More details can be found here: https://discuss.tvm.ai/t/error-reporting/7793.</span>
<span class="x">---------------------------------------------------------------</span>
<span class="x">  Check failed: bf != nullptr == false: target.build.cuda is not enabled</span>
</pre></div>
</div>
</div>
<div class="section" id="run-the-function">
<h2>Run the Function<a class="headerlink" href="#run-the-function" title="Permalink to this headline">¶</a></h2>
<p>The compiled TVM function is exposes a concise C API
that can be invoked from any language.</p>
<p>We provide a minimal array API in python to aid quick testing and prototyping.
The array API is based on the <a class="reference external" href="https://github.com/dmlc/dlpack">DLPack</a> standard.</p>
<ul class="simple">
<li><p>We first create a GPU context.</p></li>
<li><p>Then tvm.nd.array copies the data to the GPU.</p></li>
<li><p>fadd runs the actual computation.</p></li>
<li><p>asnumpy() copies the GPU array back to the CPU and we can use this to verify correctness</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">context</span><span class="p">(</span><span class="n">tgt</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">ctx</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">ctx</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">C</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">ctx</span><span class="p">)</span>
<span class="n">fadd</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="n">tvm</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">a</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="inspect-the-generated-code">
<h2>Inspect the Generated Code<a class="headerlink" href="#inspect-the-generated-code" title="Permalink to this headline">¶</a></h2>
<p>You can inspect the generated code in TVM. The result of tvm.build
is a TVM Module. fadd is the host module that contains the host wrapper,
it also contains a device module for the CUDA (GPU) function.</p>
<p>The following code fetches the device module and prints the content code.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">tgt</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">or</span> <span class="n">tgt</span> <span class="o">==</span> <span class="s2">&quot;rocm&quot;</span> <span class="ow">or</span> <span class="n">tgt</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;opencl&quot;</span><span class="p">):</span>
    <span class="n">dev_module</span> <span class="o">=</span> <span class="n">fadd</span><span class="o">.</span><span class="n">imported_modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-----GPU code-----&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dev_module</span><span class="o">.</span><span class="n">get_source</span><span class="p">())</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">fadd</span><span class="o">.</span><span class="n">get_source</span><span class="p">())</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Code Specialization</p>
<p>As you may have noticed, the declarations of A, B and C all
take the same shape argument, n. TVM will take advantage of this
to pass only a single shape argument to the kernel, as you will find in
the printed device code. This is one form of specialization.</p>
<p>On the host side, TVM will automatically generate check code
that checks the constraints in the parameters. So if you pass
arrays with different shapes into fadd, an error will be raised.</p>
<p>We can do more specializations. For example, we can write
<code class="code docutils literal notranslate"><span class="pre">n</span> <span class="pre">=</span> <span class="pre">tvm.runtime.convert(1024)</span></code> instead of <code class="code docutils literal notranslate"><span class="pre">n</span> <span class="pre">=</span> <span class="pre">te.var(&quot;n&quot;)</span></code>,
in the computation declaration. The generated function will
only take vectors with length 1024.</p>
</div>
</div>
<div class="section" id="save-compiled-module">
<h2>Save Compiled Module<a class="headerlink" href="#save-compiled-module" title="Permalink to this headline">¶</a></h2>
<p>Besides runtime compilation, we can save the compiled modules into
a file and load them back later. This is called ahead of time compilation.</p>
<p>The following code first performs the following steps:</p>
<ul class="simple">
<li><p>It saves the compiled host module into an object file.</p></li>
<li><p>Then it saves the device module into a ptx file.</p></li>
<li><p>cc.create_shared calls a compiler (gcc) to create a shared library</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tvm.contrib</span> <span class="kn">import</span> <span class="n">cc</span>
<span class="kn">from</span> <span class="nn">tvm.contrib</span> <span class="kn">import</span> <span class="n">utils</span>

<span class="n">temp</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">tempdir</span><span class="p">()</span>
<span class="n">fadd</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;myadd.o&quot;</span><span class="p">))</span>
<span class="k">if</span> <span class="n">tgt</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
    <span class="n">fadd</span><span class="o">.</span><span class="n">imported_modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;myadd.ptx&quot;</span><span class="p">))</span>
<span class="k">if</span> <span class="n">tgt</span> <span class="o">==</span> <span class="s2">&quot;rocm&quot;</span><span class="p">:</span>
    <span class="n">fadd</span><span class="o">.</span><span class="n">imported_modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;myadd.hsaco&quot;</span><span class="p">))</span>
<span class="k">if</span> <span class="n">tgt</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;opencl&quot;</span><span class="p">):</span>
    <span class="n">fadd</span><span class="o">.</span><span class="n">imported_modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;myadd.cl&quot;</span><span class="p">))</span>
<span class="n">cc</span><span class="o">.</span><span class="n">create_shared</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;myadd.so&quot;</span><span class="p">),</span> <span class="p">[</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;myadd.o&quot;</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">listdir</span><span class="p">())</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Module Storage Format</p>
<p>The CPU (host) module is directly saved as a shared library (.so).
There can be multiple customized formats of the device code.
In our example, the device code is stored in ptx, as well as a meta
data json file. They can be loaded and linked separately via import.</p>
</div>
</div>
<div class="section" id="load-compiled-module">
<h2>Load Compiled Module<a class="headerlink" href="#load-compiled-module" title="Permalink to this headline">¶</a></h2>
<p>We can load the compiled module from the file system and run the code.
The following code loads the host and device module separately and
re-links them together. We can verify that the newly loaded function works.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fadd1</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">load_module</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;myadd.so&quot;</span><span class="p">))</span>
<span class="k">if</span> <span class="n">tgt</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
    <span class="n">fadd1_dev</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">load_module</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;myadd.ptx&quot;</span><span class="p">))</span>
    <span class="n">fadd1</span><span class="o">.</span><span class="n">import_module</span><span class="p">(</span><span class="n">fadd1_dev</span><span class="p">)</span>

<span class="k">if</span> <span class="n">tgt</span> <span class="o">==</span> <span class="s2">&quot;rocm&quot;</span><span class="p">:</span>
    <span class="n">fadd1_dev</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">load_module</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;myadd.hsaco&quot;</span><span class="p">))</span>
    <span class="n">fadd1</span><span class="o">.</span><span class="n">import_module</span><span class="p">(</span><span class="n">fadd1_dev</span><span class="p">)</span>

<span class="k">if</span> <span class="n">tgt</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;opencl&quot;</span><span class="p">):</span>
    <span class="n">fadd1_dev</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">load_module</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;myadd.cl&quot;</span><span class="p">))</span>
    <span class="n">fadd1</span><span class="o">.</span><span class="n">import_module</span><span class="p">(</span><span class="n">fadd1_dev</span><span class="p">)</span>

<span class="n">fadd1</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="n">tvm</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">a</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="pack-everything-into-one-library">
<h2>Pack Everything into One Library<a class="headerlink" href="#pack-everything-into-one-library" title="Permalink to this headline">¶</a></h2>
<p>In the above example, we store the device and host code separately.
TVM also supports export everything as one shared library.
Under the hood, we pack the device modules into binary blobs and link
them together with the host code.
Currently we support packing of Metal, OpenCL and CUDA modules.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fadd</span><span class="o">.</span><span class="n">export_library</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;myadd_pack.so&quot;</span><span class="p">))</span>
<span class="n">fadd2</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">load_module</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;myadd_pack.so&quot;</span><span class="p">))</span>
<span class="n">fadd2</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="n">tvm</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">a</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Runtime API and Thread-Safety</p>
<p>The compiled modules of TVM do not depend on the TVM compiler.
Instead, they only depend on a minimum runtime library.
The TVM runtime library wraps the device drivers and provides
thread-safe and device agnostic calls into the compiled functions.</p>
<p>This means that you can call the compiled TVM functions from any thread,
on any GPUs.</p>
</div>
</div>
<div class="section" id="generate-opencl-code">
<h2>Generate OpenCL Code<a class="headerlink" href="#generate-opencl-code" title="Permalink to this headline">¶</a></h2>
<p>TVM provides code generation features into multiple backends,
we can also generate OpenCL code or LLVM code that runs on CPU backends.</p>
<p>The following code blocks generate OpenCL code, creates array on an OpenCL
device, and verifies the correctness of the code.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">tgt</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;opencl&quot;</span><span class="p">):</span>
    <span class="n">fadd_cl</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;myadd&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------opencl code------&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">fadd_cl</span><span class="o">.</span><span class="n">imported_modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_source</span><span class="p">())</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cl</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">1024</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">ctx</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">ctx</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">C</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">ctx</span><span class="p">)</span>
    <span class="n">fadd_cl</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
    <span class="n">tvm</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">a</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>This tutorial provides a walk through of TVM workflow using
a vector add example. The general workflow is</p>
<ul class="simple">
<li><p>Describe your computation via a series of operations.</p></li>
<li><p>Describe how we want to compute use schedule primitives.</p></li>
<li><p>Compile to the target function we want.</p></li>
<li><p>Optionally, save the function to be loaded later.</p></li>
</ul>
<p>You are more than welcome to checkout other examples and
tutorials to learn more about the supported operations, scheduling primitives
and other features in TVM.</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-tutorials-get-started-tensor-expr-get-started-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/37c76200603adf82ebeffc23bdef8d31/tensor_expr_get_started.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">tensor_expr_get_started.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/70a0767409e81bb5aaa9ce4e7a151dec/tensor_expr_get_started.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">tensor_expr_get_started.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="tvmc_command_line_driver.html" class="btn btn-neutral float-right" title="Getting Started with TVM command line driver - TVMC" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="relay_quick_start.html" class="btn btn-neutral float-left" title="Quick Start Tutorial for Compiling Deep Learning Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../../_static//img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <ul class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <li class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2020 Apache Software Foundation | All right reserved</h5>
        </li>
      </ul>

    </div>

    <ul>
      <li class="footernote">Copyright © 2020 The Apache Software Foundation. Apache TVM, Apache, the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.</li>
    </ul>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-75982049-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>